{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f78bd5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/indramandal/Documents/VS_CODE/DA6401/DA6401_Assignment_3/predictions_attention/env3/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /Users/indramandal/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33med24s014\u001b[0m (\u001b[33med24s014-indian-institute-of-technology-madras\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import random\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import copy\n",
    "from copy import deepcopy\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import Counter\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import wandb\n",
    "wandb.login(key = '5df7feeffbc5b918c8947f5fe4bab4b67ebfbb69')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6065adee",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df =('/Users/indramandal/Documents/VS_CODE/DA6401/DA6401_Assignment_3/predictions_attention/dakshina_dataset_v1.0/bn/lexicons/bn.translit.sampled.train.tsv')\n",
    "dev_df = ('/Users/indramandal/Documents/VS_CODE/DA6401/DA6401_Assignment_3/predictions_attention/dakshina_dataset_v1.0/bn/lexicons/bn.translit.sampled.dev.tsv')\n",
    "test_df = ('/Users/indramandal/Documents/VS_CODE/DA6401/DA6401_Assignment_3/predictions_attention/dakshina_dataset_v1.0/bn/lexicons/bn.translit.sampled.test.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3726a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Special tokens\n",
    "SOS = '<sos>'\n",
    "EOS = '<eos>'\n",
    "PAD = '<pad>'\n",
    "UNK = '<unk>'\n",
    "\n",
    "class SequenceDataPreprocessor:\n",
    "    def __init__(self, path, input_vocab=None, output_vocab=None):\n",
    "        self.path = path\n",
    "        self.input_token_to_idx = input_vocab\n",
    "        self.output_token_to_idx = output_vocab\n",
    "\n",
    "    def read_data(self, file_path):\n",
    "        df = pd.read_csv(file_path, sep=\"\\t\", names=[\"target\", \"input\", \"count\"]).astype(str)\n",
    "        inputs, outputs = [], []\n",
    "        for _, row in df.iterrows():\n",
    "            inp = list(row['input'])\n",
    "            out = [SOS] + list(row['target']) + [EOS]\n",
    "            inputs.append(inp)\n",
    "            outputs.append(out)\n",
    "        return inputs, outputs\n",
    "\n",
    "    def build_vocab(self, sequences):\n",
    "        all_tokens = [token for seq in sequences for token in seq]\n",
    "        counts = Counter(all_tokens)\n",
    "\n",
    "        specials_list = [PAD, SOS, EOS, UNK]\n",
    "        for token in specials_list:\n",
    "            counts[token] = counts.get(token, 1)\n",
    "\n",
    "        normal_tokens = sorted([tok for tok in counts if tok not in specials_list])\n",
    "        tokens = specials_list + normal_tokens\n",
    "\n",
    "        return {token: idx for idx, token in enumerate(tokens)}\n",
    "\n",
    "    def encode_sequences(self, sequences, vocab):\n",
    "        unk_idx = vocab.get(UNK, vocab.get(PAD, 0))  # Fallback\n",
    "        return [torch.tensor([vocab.get(token, unk_idx) for token in seq], dtype=torch.long) for seq in sequences]\n",
    "\n",
    "    def pad_batch(self, batch, pad_idx):\n",
    "        return pad_sequence(batch, batch_first=True, padding_value=pad_idx)\n",
    "\n",
    "    def prepare_tensors(self):\n",
    "        inputs, targets = self.read_data(self.path)\n",
    "\n",
    "        # Build vocab if not provided\n",
    "        if self.input_token_to_idx is None:\n",
    "            self.input_token_to_idx = self.build_vocab(inputs)\n",
    "        if self.output_token_to_idx is None:\n",
    "            self.output_token_to_idx = self.build_vocab(targets)\n",
    "\n",
    "        # Check PAD is in vocab\n",
    "        if PAD not in self.input_token_to_idx or PAD not in self.output_token_to_idx:\n",
    "            raise ValueError(\"PAD token not found in vocab. Ensure special tokens are added in build_vocab.\")\n",
    "\n",
    "        input_ids = self.encode_sequences(inputs, self.input_token_to_idx)\n",
    "        target_ids = self.encode_sequences(targets, self.output_token_to_idx)\n",
    "\n",
    "        input_tensor = self.pad_batch(input_ids, self.input_token_to_idx[PAD])\n",
    "        target_tensor = self.pad_batch(target_ids, self.output_token_to_idx[PAD])\n",
    "\n",
    "        return input_tensor, target_tensor, self.input_token_to_idx, self.output_token_to_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7157170a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Datasets(Dataset):\n",
    "    def __init__(self, input_tensor, target_tensor):\n",
    "        self.input_tensor = input_tensor\n",
    "        self.target_tensor = target_tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.input_tensor.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        return self.input_tensor[idx], self.target_tensor[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e2d0f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AccuracyCalculator:\n",
    "    def __init__(self, eos_token: str, pad_token: str, vocab_out: dict, device: torch.device):\n",
    "        \"\"\"\n",
    "        eos_token: the string for <eos>\n",
    "        pad_token: the string for <pad>\n",
    "        vocab_out: token->index mapping for your output vocab\n",
    "        device:     torch.device (e.g. 'cuda' or 'cpu')\n",
    "        \"\"\"\n",
    "        self.device = device\n",
    "        self.eos_idx = vocab_out[eos_token]\n",
    "        self.pad_idx = vocab_out[pad_token]\n",
    "\n",
    "    def _trim_batch_at_eos(self, sequences: torch.LongTensor):\n",
    "        \"\"\"\n",
    "        sequences: (batch_size, seq_len)\n",
    "        Returns: list of 1D LongTensors, each trimmed to include its first <eos> (if any),\n",
    "                 or the full length if no <eos> appears.\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = sequences.size()\n",
    "        trimmed = []\n",
    "        # move to CPU & numpy for easy indexing\n",
    "        seqs = sequences.detach().cpu().tolist()\n",
    "        for seq in seqs:\n",
    "            if self.eos_idx in seq:\n",
    "                end = seq.index(self.eos_idx) + 1\n",
    "                trimmed.append(torch.tensor(seq[:end], dtype=torch.long, device=self.device))\n",
    "            else:\n",
    "                trimmed.append(torch.tensor(seq, dtype=torch.long, device=self.device))\n",
    "        return trimmed\n",
    "\n",
    "    def compute_accuracy(self,\n",
    "                         predictions: torch.LongTensor,\n",
    "                         targets:     torch.LongTensor\n",
    "                         ) -> dict:\n",
    "        \"\"\"\n",
    "        predictions: (batch_size, seq_len) of token-indices, already argmaxed\n",
    "        targets:     (batch_size, seq_len) of token-indices, contains <sos>…<eos> and padding\n",
    "        \"\"\"\n",
    "        predictions = predictions.to(self.device)\n",
    "        targets     = targets.to(self.device)\n",
    "\n",
    "        batch_size, seq_len = targets.shape\n",
    "\n",
    "        # Ensure predictions and targets have the same length by trimming or padding predictions\n",
    "        if predictions.size(1) > seq_len:\n",
    "            predictions = predictions[:, :seq_len]\n",
    "        elif predictions.size(1) < seq_len:\n",
    "            pad_len = seq_len - predictions.size(1)\n",
    "            padding = torch.full((predictions.size(0), pad_len), self.pad_idx, dtype=predictions.dtype, device=predictions.device)\n",
    "            predictions = torch.cat([predictions, padding], dim=1)\n",
    "\n",
    "        # 1) Character-level accuracy (ignoring PAD completely)\n",
    "        nonpad_mask   = targets != self.pad_idx                # (B, L) bool\n",
    "        char_correct  = ((predictions == targets) & nonpad_mask).sum().item()\n",
    "        char_total    = nonpad_mask.sum().item()\n",
    "        char_accuracy = char_correct / char_total if char_total > 0 else 0.0\n",
    "\n",
    "        # 2) Sequence-level accuracy\n",
    "        #    Trim both preds & targets at each target's <eos>, then compare exactly.\n",
    "        pred_trimmed = self._trim_batch_at_eos(predictions)\n",
    "        targ_trimmed = self._trim_batch_at_eos(targets)\n",
    "\n",
    "        seq_correct = 0\n",
    "        for p_seq, t_seq in zip(pred_trimmed, targ_trimmed):\n",
    "            if p_seq.size(0) == t_seq.size(0) and torch.equal(p_seq, t_seq):\n",
    "                seq_correct += 1\n",
    "\n",
    "        seq_accuracy = seq_correct / batch_size if batch_size > 0 else 0.0\n",
    "\n",
    "        return {\n",
    "            'sequence_accuracy':   seq_accuracy,\n",
    "            'character_accuracy':  char_accuracy,\n",
    "            'correct_sequences':   seq_correct,\n",
    "            'total_sequences':     batch_size,\n",
    "            'correct_characters':  char_correct,\n",
    "            'total_characters':    char_total\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ee5d638",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, layer_type, emb_dim, hidden_layers_size, num_encod_layers, dropout_rate, pad_index, bidirectional=False):\n",
    "        super().__init__()\n",
    "        self.layer_type = layer_type\n",
    "        self.layers = self.layer_mode(layer_type)\n",
    "        self.bidirectional = bidirectional\n",
    "        self.num_encod_layers = num_encod_layers\n",
    "        self.hidden_size = hidden_layers_size\n",
    "\n",
    "        self.embed = nn.Embedding(input_size, emb_dim, padding_idx=pad_index)\n",
    "        self.layer = self.layers(\n",
    "            emb_dim, \n",
    "            hidden_layers_size, \n",
    "            num_encod_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=bidirectional,\n",
    "            dropout=dropout_rate if num_encod_layers > 1 else 0\n",
    "        )\n",
    "\n",
    "    def layer_mode(self, layer_type):\n",
    "        layer_type = layer_type.lower()\n",
    "        if layer_type == \"rnn\":\n",
    "            return nn.RNN\n",
    "        elif layer_type == \"lstm\":\n",
    "            return nn.LSTM\n",
    "        else:\n",
    "            return nn.GRU\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        embed = self.embed(input_seq)\n",
    "        if self.layer_type == \"lstm\":\n",
    "            outputs, (hidden, cell) = self.layer(embed)\n",
    "        else:\n",
    "            outputs, hidden = self.layer(embed)\n",
    "            cell = None\n",
    "        return outputs, hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7340b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, method, encoder_hidden_size, decoder_hidden_size):\n",
    "        super().__init__()\n",
    "        self.method = method\n",
    "        self.encoder_hidden_size = encoder_hidden_size\n",
    "        self.decoder_hidden_size = decoder_hidden_size\n",
    "        \n",
    "        if self.method == 'Luong_general':\n",
    "            self.Wa = nn.Linear(encoder_hidden_size, decoder_hidden_size, bias=False)\n",
    "        elif self.method == 'Bahdanau_concat':\n",
    "            self.Wa = nn.Linear(encoder_hidden_size + decoder_hidden_size, decoder_hidden_size)\n",
    "            self.v = nn.Linear(decoder_hidden_size, 1, bias=False)\n",
    "        elif self.method != 'Luong_dot':\n",
    "            raise ValueError(\"Invalid attention method\")\n",
    "    \n",
    "    def forward(self, decoder_hidden, encoder_outputs):\n",
    "        # decoder_hidden: (batch_size, decoder_hidden_size)\n",
    "        # encoder_outputs: (batch_size, src_len, encoder_hidden_size)\n",
    "        \n",
    "        if self.method == 'Luong_dot':\n",
    "            scores = torch.bmm(encoder_outputs, decoder_hidden.unsqueeze(2)).squeeze(2)\n",
    "        elif self.method == 'Luong_general':\n",
    "            transformed_encoder = self.Wa(encoder_outputs)\n",
    "            scores = torch.bmm(transformed_encoder, decoder_hidden.unsqueeze(2)).squeeze(2)\n",
    "        elif self.method == 'Bahdanau_concat':\n",
    "            decoder_hidden_expanded = decoder_hidden.unsqueeze(1).expand(-1, encoder_outputs.size(1), -1)\n",
    "            combined = torch.cat([decoder_hidden_expanded, encoder_outputs], dim=2)\n",
    "            energy = torch.tanh(self.Wa(combined))\n",
    "            scores = self.v(energy).squeeze(2)\n",
    "        \n",
    "        attention_weights = F.softmax(scores, dim=1)\n",
    "        context = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs).squeeze(1)\n",
    "        return context, attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02fc0c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size, output_size, layer_type, emb_dim, hidden_layers_size,\n",
    "                 num_decod_layers, dropout_rate, pad_index, encoder_hidden_size, \n",
    "                 bidirectional=False, attention_method='Luong_general'):\n",
    "        super().__init__()\n",
    "        self.layer_type = layer_type.lower()\n",
    "        self.bidirectional = bidirectional\n",
    "        self.num_layers = num_decod_layers\n",
    "        self.encoder_hidden_size = encoder_hidden_size\n",
    "        self.attention_method = attention_method\n",
    "        self.hidden_size = hidden_layers_size\n",
    "\n",
    "        self.embed = nn.Embedding(input_size, emb_dim, padding_idx=pad_index)\n",
    "        \n",
    "        # Attention layer\n",
    "        self.attention = Attention(\n",
    "            method=attention_method,\n",
    "            encoder_hidden_size=encoder_hidden_size,\n",
    "            decoder_hidden_size=hidden_layers_size * (2 if bidirectional else 1)\n",
    "        )\n",
    "        \n",
    "        # RNN input size: emb_dim + encoder_hidden_size (context)\n",
    "        rnn_input_size = emb_dim + encoder_hidden_size\n",
    "        \n",
    "        rnn_cls = self.layer_mode(self.layer_type)\n",
    "        self.layer = rnn_cls(\n",
    "            rnn_input_size,\n",
    "            hidden_layers_size,\n",
    "            num_decod_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=bidirectional,\n",
    "            dropout=dropout_rate if num_decod_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        rnn_output_dim = hidden_layers_size * (2 if bidirectional else 1)\n",
    "        self.fc = nn.Linear(rnn_output_dim, output_size)\n",
    "\n",
    "    def layer_mode(self, layer_type):\n",
    "        if layer_type == \"rnn\":\n",
    "            return nn.RNN\n",
    "        elif layer_type == \"lstm\":\n",
    "            return nn.LSTM\n",
    "        else:\n",
    "            return nn.GRU\n",
    "\n",
    "    def forward(self, inputs: torch.LongTensor, hidden, encoder_outputs, cell=None):\n",
    "        inputs = inputs.unsqueeze(1)  # (batch, 1)\n",
    "        embed = self.embed(inputs)     # (batch, 1, emb_dim)\n",
    "        \n",
    "        # Prepare decoder hidden state for attention\n",
    "        if self.bidirectional:\n",
    "            # Reshape for multi-layer bidirectionality\n",
    "            hidden_reshaped = hidden.view(\n",
    "                self.num_layers, 2,  # (num_layers, num_directions)\n",
    "                -1,                  # Batch size\n",
    "                self.hidden_size\n",
    "            )\n",
    "            # Concatenate last layer's forward/backward states\n",
    "            hidden_combined = torch.cat([\n",
    "                hidden_reshaped[-1, 0, :, :],  # Forward direction\n",
    "                hidden_reshaped[-1, 1, :, :]   # Backward direction\n",
    "            ], dim=1)\n",
    "        else:\n",
    "            hidden_combined = hidden[-1]  # (batch_size, hidden_size)\n",
    "        \n",
    "        context, attn_weights = self.attention(hidden_combined, encoder_outputs)\n",
    "        context = context.unsqueeze(1)\n",
    "        rnn_input = torch.cat([embed, context], dim=2)\n",
    "        \n",
    "        if self.layer_type == \"lstm\":\n",
    "            outputs, (hidden, cell) = self.layer(rnn_input, (hidden, cell))\n",
    "        else:\n",
    "            outputs, hidden = self.layer(rnn_input, hidden)\n",
    "            cell = None\n",
    "        \n",
    "        predict_word = self.fc(outputs.squeeze(1))\n",
    "        return predict_word, hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e67b482a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Sequence2Sequence(nn.Module):\n",
    "    def __init__(self, encoder, decoder, output_vocab):\n",
    "        super().__init__()\n",
    "        self.output_vocab = output_vocab\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.layer_type = encoder.layer_type.lower()\n",
    "\n",
    "    def adjust_hidden(self, hidden, desired_layers):\n",
    "        current_layers = hidden.size(0)\n",
    "        if current_layers < desired_layers:\n",
    "            zeros = torch.zeros(desired_layers - current_layers, \n",
    "                              hidden.size(1), \n",
    "                              hidden.size(2),\n",
    "                              device=hidden.device,\n",
    "                              dtype=hidden.dtype)\n",
    "            adjusted = torch.cat([hidden, zeros], dim=0)\n",
    "        else:\n",
    "            adjusted = hidden[:desired_layers]\n",
    "        return adjusted\n",
    "\n",
    "    def forward(self, input_sequence, target_sequence, teacher_force_ratio=0.5):\n",
    "        batch_size = input_sequence.size(0)\n",
    "        target_len = target_sequence.size(1)\n",
    "        outputs = torch.zeros(batch_size, target_len, len(self.output_vocab)).to(input_sequence.device)\n",
    "        \n",
    "        # Encoder forward\n",
    "        encoder_outputs, hidden, cell = self.encoder(input_sequence)\n",
    "        \n",
    "        # Handle bidirectional encoder\n",
    "        # if self.encoder.bidirectional:\n",
    "        #     encoder_outputs = encoder_outputs[:, :, :self.encoder.hidden_size] \n",
    "        \n",
    "        # Adjust hidden states for decoder\n",
    "        encoder_directions = 2 if self.encoder.bidirectional else 1\n",
    "        decoder_directions = 2 if self.decoder.bidirectional else 1\n",
    "        encoder_total = self.encoder.num_encod_layers * encoder_directions\n",
    "        decoder_total = self.decoder.num_layers * decoder_directions\n",
    "        \n",
    "        hidden = self.adjust_hidden(hidden, decoder_total)\n",
    "        cell = self.adjust_hidden(cell, decoder_total) if cell is not None else None\n",
    "        \n",
    "        x = target_sequence[:, 0]\n",
    "        for t in range(1, target_len):\n",
    "            output, hidden, cell = self.decoder(x, hidden, encoder_outputs, cell)\n",
    "            outputs[:, t] = output\n",
    "            x = target_sequence[:, t] if random.random() < teacher_force_ratio else output.argmax(1)\n",
    "        return outputs\n",
    "\n",
    "    def beam_search_decode(self, input_sequence, sos_token, eos_token, beam_width=3, max_len=30):\n",
    "        self.encoder.eval()\n",
    "        self.decoder.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            device = input_sequence.device\n",
    "            encoder_outputs, hidden, cell = self.encoder(input_sequence)\n",
    "            \n",
    "            # Handle bidirectional encoder outputs\n",
    "            # if self.encoder.bidirectional:\n",
    "            #     encoder_outputs = encoder_outputs[:, :, :self.encoder.hidden_size] + encoder_outputs[:, :, self.encoder.hidden_size:]\n",
    "            \n",
    "            # Adjust hidden states\n",
    "            encoder_directions = 2 if self.encoder.bidirectional else 1\n",
    "            decoder_directions = 2 if self.decoder.bidirectional else 1\n",
    "            encoder_total = self.encoder.num_encod_layers * encoder_directions\n",
    "            decoder_total = self.decoder.num_layers * decoder_directions\n",
    "            \n",
    "            hidden = self.adjust_hidden(hidden, decoder_total)\n",
    "            cell = self.adjust_hidden(cell, decoder_total) if cell is not None else None\n",
    "\n",
    "            beams = [([sos_token], 0.0, hidden.repeat(1, beam_width, 1), cell.repeat(1, beam_width, 1) if cell is not None else None)]\n",
    "            completed_sequences = []\n",
    "\n",
    "            for _ in range(max_len):\n",
    "                temp_beams = []\n",
    "                for seq, score, h, c in beams:\n",
    "                    if seq[-1] == eos_token:\n",
    "                        completed_sequences.append((seq, score))\n",
    "                        continue\n",
    "\n",
    "                    last_token = torch.LongTensor([seq[-1]]).to(device)\n",
    "                    out, h_new, c_new = self.decoder(last_token, h, encoder_outputs.repeat(beam_width, 1, 1), c)\n",
    "                    log_probs = torch.log_softmax(out, dim=1)\n",
    "                    top_log_probs, top_indices = torch.topk(log_probs, beam_width)\n",
    "\n",
    "                    for i in range(beam_width):\n",
    "                        token = top_indices[0][i].item()\n",
    "                        new_seq = seq + [token]\n",
    "                        new_score = score + top_log_probs[0][i].item()\n",
    "                        temp_beams.append((new_seq, new_score, h_new[:, i:i+1, :], c_new[:, i:i+1, :] if c_new is not None else None))\n",
    "\n",
    "                beams = sorted(temp_beams, key=lambda x: x[1], reverse=True)[:beam_width]\n",
    "                if all(seq[-1] == eos_token for seq, _, _, _ in beams):\n",
    "                    completed_sequences.extend(beams)\n",
    "                    break\n",
    "\n",
    "            if not completed_sequences:\n",
    "                completed_sequences = beams\n",
    "\n",
    "            best_sequence = max(completed_sequences, key=lambda x: x[1])[0]\n",
    "            return best_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a86b6b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Train_Model:\n",
    "    def __init__(self, seq2seq, dataloader, optimizer, loss_fn, acc_calculator, device):\n",
    "        self.dataloader = dataloader\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "        self.acc_calculator = acc_calculator\n",
    "        self.device = device\n",
    "        self.seq2seq = seq2seq\n",
    "\n",
    "    def count_params(self,model):\n",
    "        return sum(param.numel() for param in model.parameters() if param.requires_grad)\n",
    "\n",
    "    def train(self, teacher_force_ratio=0.5):\n",
    "        self.seq2seq.to(self.device)\n",
    "        self.seq2seq.train()\n",
    "\n",
    "        epoch_loss = 0\n",
    "        seq_acc = 0\n",
    "        character_acc = 0\n",
    "        total_seqs = 0\n",
    "        total_chars = 0\n",
    "\n",
    "        progress_bar = tqdm(self.dataloader, desc=\"Training Batches\")\n",
    "\n",
    "        for input_batch, target_batch in progress_bar:\n",
    "            input_batch = input_batch.to(self.device)\n",
    "            target_batch = target_batch.to(self.device)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            output = self.seq2seq(input_batch, target_batch, teacher_force_ratio)\n",
    "            _, predicted = torch.max(output, dim=2)\n",
    "\n",
    "            # Calculate accuracy ignoring SOS token\n",
    "            predicted_trimmed = predicted[:, 1:]\n",
    "            target_trimmed = target_batch[:, 1:]\n",
    "\n",
    "            # Flatten for loss calculation\n",
    "            output_flat = output.view(-1, output.shape[-1])\n",
    "            target_flat = target_batch.reshape(-1).to(self.device)\n",
    "\n",
    "            loss = self.loss_fn(output_flat, target_flat)\n",
    "            acc = self.acc_calculator.compute_accuracy(predicted_trimmed, target_trimmed)\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.seq2seq.parameters(), max_norm=1)\n",
    "            self.optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            seq_acc += acc['correct_sequences']\n",
    "            total_seqs += acc['total_sequences']\n",
    "            character_acc += acc['correct_characters']\n",
    "            total_chars += acc['total_characters']\n",
    "\n",
    "            avg_seq_acc = seq_acc / total_seqs if total_seqs > 0 else 0.0\n",
    "            avg_char_acc = character_acc / total_chars if total_chars > 0 else 0.0\n",
    "\n",
    "            progress_bar.set_postfix({\n",
    "                'Train_loss': loss.item(),\n",
    "                'seq_acc': f\"{avg_seq_acc:.2%}\",\n",
    "                'char_acc': f\"{avg_char_acc:.2%}\"\n",
    "            })\n",
    "\n",
    "        return (epoch_loss / len(self.dataloader), \n",
    "                avg_char_acc, \n",
    "                avg_seq_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b020ad89",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Evaluate_Model:\n",
    "    def __init__(self, seq2seq, dataloader, loss_fn, acc_calculator, device):\n",
    "        self.dataloader = dataloader\n",
    "        self.loss_fn = loss_fn\n",
    "        self.acc_calculator = acc_calculator\n",
    "        self.device = device\n",
    "        self.seq2seq = seq2seq\n",
    "\n",
    "    def evaluate(self, beam_search=False, beam_width=3):\n",
    "        self.seq2seq.eval()\n",
    "        self.seq2seq.to(self.device)\n",
    "\n",
    "        epoch_loss = 0\n",
    "        seq_acc = 0\n",
    "        character_acc = 0\n",
    "        total_seqs = 0\n",
    "        total_chars = 0\n",
    "\n",
    "        progress_bar = tqdm(self.dataloader, desc=\"Evaluation Batches\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for input_batch, target_batch in progress_bar:\n",
    "                input_batch = input_batch.to(self.device)\n",
    "                target_batch = target_batch.to(self.device)\n",
    "                \n",
    "                if beam_search:\n",
    "                    decoded_batch = []\n",
    "                    for i in range(input_batch.size(0)):\n",
    "                        predicted_ids = self.seq2seq.beam_search_decode(\n",
    "                            input_batch[i].unsqueeze(0),\n",
    "                            sos_token=self.seq2seq.output_vocab[SOS],\n",
    "                            eos_token=self.seq2seq.output_vocab[EOS],\n",
    "                            beam_width=beam_width\n",
    "                        )\n",
    "                        decoded_batch.append(torch.tensor(predicted_ids, device=self.device))\n",
    "\n",
    "                    max_len = max(len(seq) for seq in decoded_batch)\n",
    "                    predicted_tensor = torch.full((input_batch.size(0), max_len), \n",
    "                                      fill_value=self.seq2seq.output_vocab[PAD], \n",
    "                                      device=self.device)\n",
    "                    for i, seq in enumerate(decoded_batch):\n",
    "                        predicted_tensor[i, :len(seq)] = seq\n",
    "                else:\n",
    "                    output = self.seq2seq(input_batch, target_batch, teacher_force_ratio=0)\n",
    "                    _, predicted = torch.max(output, dim=2)\n",
    "                    predicted_tensor = predicted\n",
    "\n",
    "                # Pad/cut predictions to match target length\n",
    "                predicted_tensor = predicted_tensor[:, :target_batch.size(1)]\n",
    "                if predicted_tensor.size(1) < target_batch.size(1):\n",
    "                    pad = torch.full((predicted_tensor.size(0), \n",
    "                                    target_batch.size(1) - predicted_tensor.size(1)),\n",
    "                                    self.seq2seq.output_vocab[PAD], \n",
    "                                    device=self.device)\n",
    "                    predicted_tensor = torch.cat([predicted_tensor, pad], dim=1)\n",
    "\n",
    "                # Calculate loss\n",
    "                output = self.seq2seq(input_batch, target_batch, teacher_force_ratio=0)\n",
    "                output_flat = output.view(-1, output.shape[-1])\n",
    "                target_flat = target_batch.view(-1)\n",
    "                loss = self.loss_fn(output_flat, target_flat)\n",
    "\n",
    "                # Calculate accuracy\n",
    "                pred_trimmed = predicted_tensor[:, 1:]\n",
    "                target_trimmed = target_batch[:, 1:]\n",
    "                acc = self.acc_calculator.compute_accuracy(pred_trimmed, target_trimmed)\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "                seq_acc += acc['correct_sequences']\n",
    "                total_seqs += acc['total_sequences']\n",
    "                character_acc += acc['correct_characters']\n",
    "                total_chars += acc['total_characters']\n",
    "\n",
    "                avg_seq_acc = seq_acc / total_seqs if total_seqs > 0 else 0.0\n",
    "                avg_char_acc = character_acc / total_chars if total_chars > 0 else 0.0\n",
    "\n",
    "                progress_bar.set_postfix({\n",
    "                    'Val_loss': loss.item(),\n",
    "                    'seq_acc': f\"{avg_seq_acc:.2%}\",\n",
    "                    'char_acc': f\"{avg_char_acc:.2%}\"\n",
    "                })\n",
    "\n",
    "        return (epoch_loss / len(self.dataloader), \n",
    "                avg_char_acc, \n",
    "                avg_seq_acc)\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "730c4b7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS device\n"
     ]
    }
   ],
   "source": [
    "# Check if mps is available\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS device\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU device\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a14c2f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "590bf0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "75470420",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Build_Model:\n",
    "    def __init__(self, \n",
    "                 sequence_data_preprocessor,\n",
    "                 encoder_class, \n",
    "                 decoder_class, \n",
    "                 seq2seq_class, \n",
    "                 attention_class,\n",
    "                 batch_size,\n",
    "                 train_path,\n",
    "                 val_path,\n",
    "                 device=\"cpu\"):\n",
    "        \n",
    "        self.sequence_data_preprocessor = sequence_data_preprocessor\n",
    "        self.encoder_class = encoder_class\n",
    "        self.decoder_class = decoder_class\n",
    "        self.seq2seq_class = seq2seq_class\n",
    "        self.attention_class = attention_class\n",
    "        self.batch_size = batch_size\n",
    "        self.train_path = train_path\n",
    "        self.val_path = val_path\n",
    "        self.device = device\n",
    "\n",
    "        # Process datasets\n",
    "        train_processor = self.sequence_data_preprocessor(self.train_path)\n",
    "        self.train_input_tensor, self.train_target_tensor, self.input_vocab, self.output_vocab = train_processor.prepare_tensors()\n",
    "\n",
    "        val_processor = self.sequence_data_preprocessor(self.val_path, input_vocab=self.input_vocab, output_vocab=self.output_vocab)\n",
    "        self.val_input_tensor, self.val_target_tensor, _, _ = val_processor.prepare_tensors()\n",
    "\n",
    "        # Create datasets and dataloaders\n",
    "        train_data = Datasets(self.train_input_tensor, self.train_target_tensor)\n",
    "        val_data = Datasets(self.val_input_tensor, self.val_target_tensor)\n",
    "\n",
    "        self.train_dataloader = DataLoader(train_data, batch_size=self.batch_size, shuffle=True)\n",
    "        self.val_dataloader = DataLoader(val_data, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "    def epoch_time(self, start_time, end_time):\n",
    "        elapsed = end_time - start_time\n",
    "        return int(elapsed // 60), int(elapsed % 60)\n",
    "\n",
    "    def build(self,\n",
    "              emb_size, layer_type, hidden_layers_size,\n",
    "              num_encod_layers, num_decod_layers,\n",
    "              dropout_rate, epochs, learning_rate,\n",
    "              teacher_force_ratio=0, bidirectional=False,\n",
    "              save_path='best_model.pt', patience=3,\n",
    "              val_beam_search=False, beam_width=3,\n",
    "              testing_phase=False, test_path=None,\n",
    "              test_beam_search=False, wandb_log=False,\n",
    "              attention_method='Luong_general'):\n",
    "        \n",
    "        # Instantiate Encoder\n",
    "        encoder = self.encoder_class(\n",
    "            input_size=len(self.input_vocab),\n",
    "            layer_type=layer_type,\n",
    "            emb_dim=emb_size,\n",
    "            hidden_layers_size=hidden_layers_size,\n",
    "            num_encod_layers=num_encod_layers,\n",
    "            dropout_rate=dropout_rate,\n",
    "            pad_index=self.input_vocab[PAD],\n",
    "            bidirectional=bidirectional\n",
    "        ).to(self.device)\n",
    "\n",
    "        # Calculate encoder hidden size for attention\n",
    "        encoder_hidden_size = hidden_layers_size * (2 if bidirectional else 1)\n",
    "\n",
    "        # Instantiate Decoder with Attention\n",
    "        decoder = self.decoder_class(\n",
    "            input_size=len(self.output_vocab),\n",
    "            output_size=len(self.output_vocab),\n",
    "            layer_type=layer_type,\n",
    "            emb_dim=emb_size,\n",
    "            hidden_layers_size=hidden_layers_size,\n",
    "            num_decod_layers=num_decod_layers,\n",
    "            dropout_rate=dropout_rate,\n",
    "            pad_index=self.output_vocab[PAD],\n",
    "            encoder_hidden_size=encoder_hidden_size,\n",
    "            bidirectional=bidirectional,\n",
    "            attention_method=attention_method\n",
    "        ).to(self.device)\n",
    "\n",
    "        # Assemble Seq2Seq Model\n",
    "        seq2seq = self.seq2seq_class(\n",
    "            encoder=encoder,\n",
    "            decoder=decoder,\n",
    "            output_vocab=self.output_vocab\n",
    "        ).to(self.device)\n",
    "\n",
    "        optimizer = optim.Adam(seq2seq.parameters(), lr=learning_rate)\n",
    "        criterion = nn.CrossEntropyLoss(ignore_index=self.output_vocab[PAD])\n",
    "\n",
    "        acc_calculator = AccuracyCalculator(\n",
    "            eos_token=EOS,\n",
    "            pad_token=PAD,\n",
    "            vocab_out=self.output_vocab,\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "        train_model = Train_Model(seq2seq, self.train_dataloader, optimizer, criterion, acc_calculator, self.device)\n",
    "        evaluate_model = Evaluate_Model(seq2seq, self.val_dataloader, criterion, acc_calculator, self.device)\n",
    "\n",
    "        print(f'The model has {train_model.count_params(seq2seq):,} trainable parameters')\n",
    "\n",
    "        best_val_loss = float('inf')\n",
    "        best_model_state = None\n",
    "        epochs_no_improve = 0\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            start_time = time.time()\n",
    "\n",
    "            # Decaying teacher forcing ratio\n",
    "            decay_rate = 0.05\n",
    "            current_tfr = teacher_force_ratio * np.exp(-decay_rate * epoch)\n",
    "            current_tfr = max(0.0, current_tfr)\n",
    "\n",
    "            print(f'\\nEpoch {epoch+1}/{epochs} | Teacher Forcing Ratio: {current_tfr:.4f}\\n{\"-\"*80}')\n",
    "            train_loss, train_char_acc, train_seq_acc = train_model.train(teacher_force_ratio=current_tfr)\n",
    "            print(f'Train Loss: {train_loss:.4f} | Char Acc: {train_char_acc:.4f} | Seq Acc: {train_seq_acc:.4f}')\n",
    "\n",
    "            val_loss, val_char_acc, val_seq_acc = evaluate_model.evaluate(beam_search=val_beam_search, beam_width=beam_width)\n",
    "            print(f'Val   Loss: {val_loss:.4f} | Char Acc: {val_char_acc:.4f} | Seq Acc: {val_seq_acc:.4f}')\n",
    "\n",
    "            if wandb_log:\n",
    "                wandb.log({\n",
    "                    'epoch': epoch + 1,\n",
    "                    'train_loss': train_loss,\n",
    "                    'train_char_acc': train_char_acc,\n",
    "                    'train_seq_acc': train_seq_acc,\n",
    "                    'val_loss': val_loss,\n",
    "                    'val_char_acc': val_char_acc,\n",
    "                    'val_seq_acc': val_seq_acc\n",
    "                })\n",
    "\n",
    "            # Early stopping and model saving\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_model_state = copy.deepcopy(seq2seq.state_dict())\n",
    "                best_train_loss = train_loss\n",
    "                best_train_char_acc = train_char_acc\n",
    "                best_train_seq_acc = train_seq_acc\n",
    "                best_val_char_acc = val_char_acc\n",
    "                best_val_seq_acc = val_seq_acc\n",
    "                epochs_no_improve = 0\n",
    "                print(\"Validation improved.\")\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "                print(f'No improvement. Patience counter: {epochs_no_improve}/{patience}')\n",
    "                if epochs_no_improve >= patience:\n",
    "                    print(f'Early stopping triggered.')\n",
    "                    break\n",
    "\n",
    "            epoch_mins, epoch_secs = self.epoch_time(start_time, time.time())\n",
    "            print(f\"Epoch Time: {epoch_mins}m {epoch_secs}s\")\n",
    "\n",
    "        # Save the best model\n",
    "        if best_model_state is not None:\n",
    "            print(\"Saving best model...\")\n",
    "            torch.save(best_model_state, save_path)\n",
    "        else:\n",
    "            print(\"Saving last model (no improvement).\")\n",
    "            torch.save(seq2seq.state_dict(), save_path)\n",
    "\n",
    "        # ========== TEST PHASE ==========\n",
    "        if testing_phase:\n",
    "            if not test_path:\n",
    "                raise ValueError(\"Test path must be provided when testing_phase=True.\")\n",
    "            \n",
    "            print(f'\\n\\n{\"+\"*28}<Testing Phase Started>{\"+\"*28}\\nPreparing test dataset...\\n')\n",
    "            test_processor = self.sequence_data_preprocessor(test_path, input_vocab=self.input_vocab, output_vocab=self.output_vocab)\n",
    "            test_input_tensor, test_target_tensor, _, _ = test_processor.prepare_tensors()\n",
    "\n",
    "            test_data = Datasets(test_input_tensor, test_target_tensor)\n",
    "            test_dataloader = DataLoader(test_data, batch_size=self.batch_size)\n",
    "\n",
    "            # Load the best model and evaluate\n",
    "            best_seq2seq = self.seq2seq_class(\n",
    "                encoder=encoder,\n",
    "                decoder=decoder,\n",
    "                output_vocab=self.output_vocab\n",
    "            ).to(self.device)\n",
    "            best_seq2seq.load_state_dict(torch.load(save_path))\n",
    "            best_seq2seq.eval()\n",
    "\n",
    "            test_eval_model = Evaluate_Model(\n",
    "                seq2seq=best_seq2seq,\n",
    "                dataloader=test_dataloader,\n",
    "                loss_fn=criterion,\n",
    "                acc_calculator=acc_calculator,\n",
    "                device=self.device\n",
    "            )\n",
    "\n",
    "            test_loss, test_char_acc, test_seq_acc = test_eval_model.evaluate(\n",
    "                beam_search=test_beam_search, \n",
    "                beam_width=beam_width\n",
    "            )\n",
    "            print(f'Test Loss: {test_loss:.4f} | Test Char Acc: {test_char_acc:.4f} | Test Seq Acc: {test_seq_acc:.4f}')\n",
    "\n",
    "        # Final log return\n",
    "        log = {\n",
    "            'train_loss': best_train_loss,\n",
    "            'train_char_acc': best_train_char_acc,\n",
    "            'train_seq_acc': best_train_seq_acc,\n",
    "            'val_loss': best_val_loss,\n",
    "            'val_char_acc': best_val_char_acc,\n",
    "            'val_seq_acc': best_val_seq_acc\n",
    "        }\n",
    "\n",
    "        if testing_phase:\n",
    "            log.update({\n",
    "                'test_loss': test_loss, 'test_char_acc': test_char_acc, 'test_seq_acc': test_seq_acc\n",
    "            })\n",
    "\n",
    "        model_params = {\n",
    "            \"input_vocab\": self.input_vocab,\n",
    "            \"output_vocab\": self.output_vocab,\n",
    "            \"emb_size\": emb_size,\n",
    "            \"layer_type\": layer_type,\n",
    "            \"hidden_layers_size\": hidden_layers_size,\n",
    "            \"num_encod_layers\": num_encod_layers,\n",
    "            \"num_decod_layers\": num_decod_layers,\n",
    "            \"dropout_rate\": dropout_rate,\n",
    "            \"learning_rate\": learning_rate,\n",
    "            \"epochs\": epochs,\n",
    "            \"teacher_force_ratio\": teacher_force_ratio,\n",
    "            \"bidirectional\": bidirectional,\n",
    "            \"beam_width\": beam_width,\n",
    "            \"attention_method\": attention_method,\n",
    "            \"patience\": patience,\n",
    "            \"save_path\": save_path\n",
    "        }\n",
    "\n",
    "        return seq2seq, train_model, evaluate_model, log, model_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "12145ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 179,648 trainable parameters\n",
      "\n",
      "Epoch 1/1 | Teacher Forcing Ratio: 1.0000\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|██████████| 2955/2955 [01:20<00:00, 36.81it/s, Train_loss=1.54, seq_acc=4.06%, char_acc=53.90%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.0847 | Char Acc: 0.5390 | Seq Acc: 0.0406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation Batches: 100%|██████████| 290/290 [00:13<00:00, 21.28it/s, Val_loss=2.33, seq_acc=8.74%, char_acc=48.92%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val   Loss: 2.3325 | Char Acc: 0.4892 | Seq Acc: 0.0874\n",
      "Validation improved.\n",
      "Epoch Time: 1m 33s\n",
      "Saving best model...\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++++++<Testing Phase Started>++++++++++++++++++++++++++++\n",
      "Preparing test dataset...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation Batches: 100%|██████████| 289/289 [00:13<00:00, 21.48it/s, Val_loss=3.15, seq_acc=9.96%, char_acc=48.82%] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 2.3491 | Test Char Acc: 0.4882 | Test Seq Acc: 0.0996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Set beam width (1 = greedy decoding, >1 = beam search)\n",
    "beam_width = 3\n",
    "beam_search = beam_width > 1\n",
    "\n",
    "# Initialize the model with attention\n",
    "model = Build_Model(\n",
    "    sequence_data_preprocessor=SequenceDataPreprocessor,\n",
    "    encoder_class=Encoder,\n",
    "    decoder_class=Decoder,\n",
    "    seq2seq_class=Sequence2Sequence,\n",
    "    attention_class=Attention,  # <-- Added attention class\n",
    "    batch_size=32,\n",
    "    train_path=train_df,\n",
    "    val_path=dev_df,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Build and train the model\n",
    "seq2seq, train_model, evaluate_model, loss_acc_logs, _ = model.build(\n",
    "    emb_size= 64,\n",
    "    layer_type=\"gru\",  # Options: \"rnn\", \"lstm\", \"gru\"\n",
    "    hidden_layers_size=64,\n",
    "    num_encod_layers=1,\n",
    "    num_decod_layers=1,\n",
    "    dropout_rate=0.3,\n",
    "    epochs=1,\n",
    "    learning_rate=0.0001,\n",
    "    teacher_force_ratio= 1,\n",
    "    bidirectional= True,\n",
    "    patience=3,\n",
    "    val_beam_search= True,  # Uses beam search if beam_width > 1\n",
    "    beam_width= 1,\n",
    "    testing_phase=True,          # Set to True if test phase is needed\n",
    "    test_path=test_df,\n",
    "    test_beam_search=True,\n",
    "    wandb_log=False,\n",
    "    attention_method= \"Luong_general\"  # Options: \"Luong_general\", \"Bahdanau_concat\", \"Luong_dot\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d260ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    'method': 'bayes',\n",
    "    'metric': {\n",
    "        'name': 'val_loss',\n",
    "        'goal': 'minimize'\n",
    "    },\n",
    "    'parameters': {\n",
    "        'emb_size': {'values': [16, 32, 64, 256]},\n",
    "        'hidden_size': {'values': [32, 64, 256, 512]},\n",
    "        'layer_type': {'values': ['rnn', 'gru', 'lstm']},\n",
    "        'enc_layers': {'values': [1, 2 ,3]},\n",
    "        'dec_layers': {'values': [1, 2, 3]},\n",
    "        'dropout': {'values': [0.2, 0.3]},\n",
    "        'learning_rate': {'values': [1e-4, 5e-4, 1e-3]},\n",
    "        'teacher_force_ratio': {'values': [0.3, 0.5, 0.7, 1.0]},\n",
    "        'epochs': {'value': 30},\n",
    "        'bidirectional': {'values': [True, False]},\n",
    "        'beam_width': {'values': [1, 3, 5]},\n",
    "        'val_beam_search': {'values': [True , False]},\n",
    "        'attention_method': {'values': ['Luong_dot', 'Luong_general', 'Bahdanau_concat']}  \n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d06c5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_wandb_model():\n",
    "    with wandb.init() as run:\n",
    "        config = wandb.config  # wandb handles this automatically during sweeps\n",
    "\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "        run.name = (\n",
    "            f\"{config.layer_type}/emb{config.emb_size}-hid{config.hidden_size}-\"\n",
    "            f\"enc{config.enc_layers}-dec{config.dec_layers}-\"\n",
    "            f\"{'bi' if config.bidirectional else 'uni'}-drop{int(config.dropout * 100)}-\"\n",
    "            f\"{timestamp}\"\n",
    "        )\n",
    "        run.save()\n",
    "\n",
    "        model = Build_Model(\n",
    "            sequence_data_preprocessor=SequenceDataPreprocessor,\n",
    "            encoder=Encoder,\n",
    "            decoder=Decoder,\n",
    "            seq2seq=Sequence2Sequence,\n",
    "            attention=Attention,\n",
    "            batch_size=32,\n",
    "            train_path=train_df,\n",
    "            val_path=dev_df,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        seq2seq, train_model, evaluate_model, loss_acc_logs, _ = model.build(\n",
    "            emb_size=config.emb_size,\n",
    "            layer_type=config.layer_type,\n",
    "            hidden_layers_size=config.hidden_size,\n",
    "            num_encod_layers=config.enc_layers,\n",
    "            num_decod_layers=config.dec_layers,\n",
    "            dropout_rate=config.dropout,\n",
    "            epochs=config.epochs,\n",
    "            learning_rate=config.learning_rate,\n",
    "            teacher_force_ratio=config.teacher_force_ratio,\n",
    "            bidirectional=config.bidirectional,\n",
    "            patience=3,\n",
    "            val_beam_search=config.val_beam_search,\n",
    "            beam_width=config.beam_width,\n",
    "            testing_phase=False,\n",
    "            test_path=None,\n",
    "            test_beam_search=False,\n",
    "            wandb_log= True,\n",
    "            attention_method= config.attention_method\n",
    "        )\n",
    "\n",
    "        # Start watching after init\n",
    "        wandb.watch(seq2seq, log=\"all\", log_freq=100)\n",
    "\n",
    "        wandb.log({\n",
    "            \"final_train_loss\": loss_acc_logs[\"train_loss\"],\n",
    "            \"final_val_loss\": loss_acc_logs[\"val_loss\"],\n",
    "            \"final_train_char_acc\": loss_acc_logs[\"train_char_acc\"],\n",
    "            \"final_train_seq_acc\": loss_acc_logs[\"train_seq_acc\"],\n",
    "            \"final_val_char_acc\": loss_acc_logs[\"val_char_acc\"],\n",
    "            \"final_val_seq_acc\": loss_acc_logs[\"val_seq_acc\"],\n",
    "        })\n",
    "\n",
    "        wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2d955ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: 90gnl1tc\n",
      "Sweep URL: https://wandb.ai/ed24s014-indian-institute-of-technology-madras/test_seq2seq_without_Attention/sweeps/90gnl1tc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: he2nq8tl with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_width: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbidirectional: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_layers: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \temb_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_layers: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlayer_type: rnn\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tteacher_force_ratio: 0.7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tval_beam_search: False\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/indramandal/Documents/VS_CODE/DA6401/DA6401_Assignment_3/predictions_vanilla/wandb/run-20250504_224310-he2nq8tl</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ed24s014-indian-institute-of-technology-madras/test_seq2seq_without_Attention/runs/he2nq8tl' target=\"_blank\">chocolate-sweep-1</a></strong> to <a href='https://wandb.ai/ed24s014-indian-institute-of-technology-madras/test_seq2seq_without_Attention' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/ed24s014-indian-institute-of-technology-madras/test_seq2seq_without_Attention/sweeps/90gnl1tc' target=\"_blank\">https://wandb.ai/ed24s014-indian-institute-of-technology-madras/test_seq2seq_without_Attention/sweeps/90gnl1tc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ed24s014-indian-institute-of-technology-madras/test_seq2seq_without_Attention' target=\"_blank\">https://wandb.ai/ed24s014-indian-institute-of-technology-madras/test_seq2seq_without_Attention</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/ed24s014-indian-institute-of-technology-madras/test_seq2seq_without_Attention/sweeps/90gnl1tc' target=\"_blank\">https://wandb.ai/ed24s014-indian-institute-of-technology-madras/test_seq2seq_without_Attention/sweeps/90gnl1tc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ed24s014-indian-institute-of-technology-madras/test_seq2seq_without_Attention/runs/he2nq8tl' target=\"_blank\">https://wandb.ai/ed24s014-indian-institute-of-technology-madras/test_seq2seq_without_Attention/runs/he2nq8tl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 158,240 trainable parameters\n",
      "\n",
      "Epoch 1/3                                        Teacher Forcing Ratio: 0.7000\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|██████████| 2955/2955 [00:21<00:00, 137.06it/s, loss=2.93]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3.0997 | Train Char Acc: 0.2024 | Train Seq Acc: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation Batches: 100%|██████████| 290/290 [00:01<00:00, 220.39it/s, loss=3.19]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val   Loss: 3.2022 | Val Char Acc: 0.1941 | Val Seq Acc: 0.0000\n",
      "Validation improved, but waiting to confirm best over next 3 epochs...\n",
      "\n",
      "Epoch Time: 0.0m 22.879174947738647s\n",
      "\n",
      "Epoch 2/3                                        Teacher Forcing Ratio: 0.6659\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|██████████| 2955/2955 [00:21<00:00, 136.33it/s, loss=3]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.9924 | Train Char Acc: 0.2185 | Train Seq Acc: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation Batches: 100%|██████████| 290/290 [00:01<00:00, 226.12it/s, loss=3.06]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val   Loss: 3.2254 | Val Char Acc: 0.1930 | Val Seq Acc: 0.0000\n",
      " No improvement. Patience: 1/3\n",
      "\n",
      "Epoch Time: 0.0m 22.960638761520386s\n",
      "\n",
      "Epoch 3/3                                        Teacher Forcing Ratio: 0.6334\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|██████████| 2955/2955 [00:21<00:00, 137.44it/s, loss=2.95]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.9746 | Train Char Acc: 0.2236 | Train Seq Acc: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation Batches: 100%|██████████| 290/290 [00:01<00:00, 223.81it/s, loss=3.19]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val   Loss: 3.2187 | Val Char Acc: 0.1930 | Val Seq Acc: 0.0000\n",
      " No improvement. Patience: 2/3\n",
      "\n",
      "Epoch Time: 0.0m 22.79924726486206s\n",
      "\n",
      "Training ended before confirming best model due to patience.\n",
      "\n",
      "++++++++++++++++++++++++<Training Ended after 3 Epochs>++++++++++++++++++++++++\n",
      "\n",
      " No test evaluation triggered. To evaluate, set `testing_phase=True`.\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▅█</td></tr><tr><td>final_train_char_acc</td><td>▁</td></tr><tr><td>final_train_loss</td><td>▁</td></tr><tr><td>final_train_seq_acc</td><td>▁</td></tr><tr><td>final_val_char_acc</td><td>▁</td></tr><tr><td>final_val_loss</td><td>▁</td></tr><tr><td>final_val_seq_acc</td><td>▁</td></tr><tr><td>train_char_acc</td><td>▁▆█</td></tr><tr><td>train_loss</td><td>█▂▁</td></tr><tr><td>train_seq_acc</td><td>▁▁█</td></tr><tr><td>val_char_acc</td><td>█▁▁</td></tr><tr><td>val_loss</td><td>▁█▆</td></tr><tr><td>val_seq_acc</td><td>▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>3</td></tr><tr><td>final_train_char_acc</td><td>0.2236</td></tr><tr><td>final_train_loss</td><td>2.97464</td></tr><tr><td>final_train_seq_acc</td><td>1e-05</td></tr><tr><td>final_val_char_acc</td><td>0.19295</td></tr><tr><td>final_val_loss</td><td>3.21869</td></tr><tr><td>final_val_seq_acc</td><td>0</td></tr><tr><td>train_char_acc</td><td>0.2236</td></tr><tr><td>train_loss</td><td>2.97464</td></tr><tr><td>train_seq_acc</td><td>1e-05</td></tr><tr><td>val_char_acc</td><td>0.19295</td></tr><tr><td>val_loss</td><td>3.21869</td></tr><tr><td>val_seq_acc</td><td>0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">rnn/emb16-hid256-enc1-dec1-uni-drop30-20250504-224311</strong> at: <a href='https://wandb.ai/ed24s014-indian-institute-of-technology-madras/test_seq2seq_without_Attention/runs/he2nq8tl' target=\"_blank\">https://wandb.ai/ed24s014-indian-institute-of-technology-madras/test_seq2seq_without_Attention/runs/he2nq8tl</a><br> View project at: <a href='https://wandb.ai/ed24s014-indian-institute-of-technology-madras/test_seq2seq_without_Attention' target=\"_blank\">https://wandb.ai/ed24s014-indian-institute-of-technology-madras/test_seq2seq_without_Attention</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250504_224310-he2nq8tl/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sweep_id = wandb.sweep(sweep_config, project=\"test_seq2seq_without_Attention\")\n",
    "wandb.agent(sweep_id, function=train_wandb_model, count=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "13402f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc84dc7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
