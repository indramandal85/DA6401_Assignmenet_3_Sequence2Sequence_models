{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f78bd5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/indramandal/Documents/VS_CODE/DA6401/DA6401_Assignment_3/predictions_vanilla/env3/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /Users/indramandal/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33med24s014\u001b[0m (\u001b[33med24s014-indian-institute-of-technology-madras\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import random\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import copy\n",
    "from copy import deepcopy\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import Counter\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import wandb\n",
    "wandb.login(key = '5df7feeffbc5b918c8947f5fe4bab4b67ebfbb69')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6065adee",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df =('/Users/indramandal/Documents/VS_CODE/DA6401/DA6401_Assignment_3/predictions_vanilla/dakshina_dataset_v1.0/bn/lexicons/bn.translit.sampled.train.tsv')\n",
    "dev_df = ('/Users/indramandal/Documents/VS_CODE/DA6401/DA6401_Assignment_3/predictions_vanilla/dakshina_dataset_v1.0/bn/lexicons/bn.translit.sampled.dev.tsv')\n",
    "test_df = ('/Users/indramandal/Documents/VS_CODE/DA6401/DA6401_Assignment_3/predictions_vanilla/dakshina_dataset_v1.0/bn/lexicons/bn.translit.sampled.test.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3726a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Special tokens\n",
    "SOS = '<sos>'\n",
    "EOS = '<eos>'\n",
    "PAD = '<pad>'\n",
    "UNK = '<unk>'\n",
    "\n",
    "class SequenceDataPreprocessor:\n",
    "    def __init__(self, path, input_vocab=None, output_vocab=None):\n",
    "        self.path = path\n",
    "        self.input_token_to_idx = input_vocab\n",
    "        self.output_token_to_idx = output_vocab\n",
    "\n",
    "    def read_data(self, file_path):\n",
    "        df = pd.read_csv(file_path, sep=\"\\t\", names=[\"target\", \"input\", \"count\"]).astype(str)\n",
    "        inputs, outputs = [], []\n",
    "        for _, row in df.iterrows():\n",
    "            inp = list(row['input'])\n",
    "            out = [SOS] + list(row['target']) + [EOS]\n",
    "            inputs.append(inp)\n",
    "            outputs.append(out)\n",
    "        return inputs, outputs\n",
    "\n",
    "    def build_vocab(self, sequences):\n",
    "        all_tokens = [token for seq in sequences for token in seq]\n",
    "        counts = Counter(all_tokens)\n",
    "\n",
    "        specials_list = [PAD, SOS, EOS, UNK]\n",
    "        for token in specials_list:\n",
    "            counts[token] = counts.get(token, 1)\n",
    "\n",
    "        normal_tokens = sorted([tok for tok in counts if tok not in specials_list])\n",
    "        tokens = specials_list + normal_tokens\n",
    "\n",
    "        return {token: idx for idx, token in enumerate(tokens)}\n",
    "\n",
    "    def encode_sequences(self, sequences, vocab):\n",
    "        unk_idx = vocab.get(UNK, vocab.get(PAD, 0))  # Fallback\n",
    "        return [torch.tensor([vocab.get(token, unk_idx) for token in seq], dtype=torch.long) for seq in sequences]\n",
    "\n",
    "    def pad_batch(self, batch, pad_idx):\n",
    "        return pad_sequence(batch, batch_first=True, padding_value=pad_idx)\n",
    "\n",
    "    def prepare_tensors(self):\n",
    "        inputs, targets = self.read_data(self.path)\n",
    "\n",
    "        # Build vocab if not provided\n",
    "        if self.input_token_to_idx is None:\n",
    "            self.input_token_to_idx = self.build_vocab(inputs)\n",
    "        if self.output_token_to_idx is None:\n",
    "            self.output_token_to_idx = self.build_vocab(targets)\n",
    "\n",
    "        # Check PAD is in vocab\n",
    "        if PAD not in self.input_token_to_idx or PAD not in self.output_token_to_idx:\n",
    "            raise ValueError(\"PAD token not found in vocab. Ensure special tokens are added in build_vocab.\")\n",
    "\n",
    "        input_ids = self.encode_sequences(inputs, self.input_token_to_idx)\n",
    "        target_ids = self.encode_sequences(targets, self.output_token_to_idx)\n",
    "\n",
    "        input_tensor = self.pad_batch(input_ids, self.input_token_to_idx[PAD])\n",
    "        target_tensor = self.pad_batch(target_ids, self.output_token_to_idx[PAD])\n",
    "\n",
    "        return input_tensor, target_tensor, self.input_token_to_idx, self.output_token_to_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7157170a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Datasets(Dataset):\n",
    "    def __init__(self, input_tensor, target_tensor):\n",
    "        self.input_tensor = input_tensor\n",
    "        self.target_tensor = target_tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.input_tensor.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        return self.input_tensor[idx], self.target_tensor[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3a862f6",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'target_tensor'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_data \u001b[38;5;241m=\u001b[39m \u001b[43mDatasets\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m val_data \u001b[38;5;241m=\u001b[39m Datasets(dev_df)\n\u001b[1;32m      3\u001b[0m test_data \u001b[38;5;241m=\u001b[39m Datasets(test_df)\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'target_tensor'"
     ]
    }
   ],
   "source": [
    "train_data = Datasets(train_df)\n",
    "val_data = Datasets(dev_df)\n",
    "test_data = Datasets(test_df)\n",
    "\n",
    "train_data[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64b6c6ce",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'target_tensor'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_data \u001b[38;5;241m=\u001b[39m \u001b[43mDatasets\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m val_data \u001b[38;5;241m=\u001b[39m Datasets(dev_df)\n\u001b[1;32m      3\u001b[0m test_data \u001b[38;5;241m=\u001b[39m Datasets(test_df)\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'target_tensor'"
     ]
    }
   ],
   "source": [
    "train_data = Datasets(train_df)\n",
    "val_data = Datasets(dev_df)\n",
    "test_data = Datasets(test_df)\n",
    "\n",
    "# Create DataLoader\n",
    "train_dataloader = DataLoader(train_data, batch_size=16, shuffle=True)\n",
    "val_dataloader = DataLoader(val_data, batch_size=16, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=len(test_data), shuffle=False)\n",
    "\n",
    "for input_batch, target_batch in train_dataloader:\n",
    "    print(\"Input batch shape:\", input_batch.shape)\n",
    "    print(\"Target batch shape:\", target_batch.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec1a50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Encoder(nn.Module):\n",
    "#     def __init__(self, input_size, layer_type, emb_dim, hidden_layers_size, num_encod_layers, dropout_rate, pad_index, bidirectional=False):\n",
    "#         super().__init__()\n",
    "\n",
    "#         self.layer_type = layer_type\n",
    "#         self.layers = self.layer_mode(layer_type)\n",
    "\n",
    "#         self.embed = nn.Embedding(input_size, emb_dim, padding_idx=pad_index)\n",
    "#         self.layer = self.layers(\n",
    "#             emb_dim, \n",
    "#             hidden_layers_size, \n",
    "#             num_encod_layers, \n",
    "#             batch_first=True,\n",
    "#             bidirectional=bidirectional, \n",
    "#             dropout=dropout_rate if num_encod_layers > 1 else 0  # Dropout is applied between layers except last\n",
    "#         )\n",
    "\n",
    "\n",
    "#     def layer_mode(self, layer_type):\n",
    "#         layer_type = layer_type.lower()\n",
    "#         if layer_type == \"rnn\":\n",
    "#             return nn.RNN\n",
    "#         elif layer_type == \"lstm\":\n",
    "#             return nn.LSTM\n",
    "#         else:\n",
    "#             return nn.GRU\n",
    "\n",
    "#     def forward(self, input_seq):\n",
    "#         embed = self.embed(input_seq)  # [batch_size, seq_len, emb_dim]\n",
    "#         if torch.isnan(embed).any() or torch.isinf(embed).any():\n",
    "#             print(\"NaN/Inf in encoder embedding:\", embed)\n",
    "#             raise AssertionError(\"NaN or Inf detected in encoder embedding\")\n",
    "\n",
    "#         try:\n",
    "#             if self.layer_type == \"lstm\":\n",
    "#                 outputs, (hidden, cell) = self.layer(embed)\n",
    "#             else:\n",
    "#                 outputs, hidden = self.layer(embed)\n",
    "#                 cell = None\n",
    "#         except Exception as e:\n",
    "#             print(\"Encoder RNN error:\", e)\n",
    "#             print(\"Input shape:\", embed.shape)\n",
    "#             raise\n",
    "\n",
    "#         # if torch.isnan(outputs).any() or torch.isinf(outputs).any():\n",
    "#         #     print(\"NaN/Inf in encoder outputs:\", outputs)\n",
    "#         #     raise AssertionError(\"NaN or Inf detected in encoder outputs\")\n",
    "#         # if torch.isnan(hidden).any() or torch.isinf(hidden).any():\n",
    "#         #     print(\"Hidden NaN/Inf:\", hidden)\n",
    "#         #     raise AssertionError(\"NaN or Inf detected in encoder hidden state\")\n",
    "#         # if self.layer_type == \"lstm\" and (torch.isnan(cell).any() or torch.isinf(cell).any()):\n",
    "#         #     print(\"Cell NaN/Inf:\", cell)\n",
    "#         #     raise AssertionError(\"NaN or Inf detected in encoder cell state\")\n",
    "\n",
    "#         return hidden, cell\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5356e0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# class Decoder(nn.Module):\n",
    "#     def __init__(self, input_size, output_size, layer_type, emb_dim, hidden_layers_size,\n",
    "#                  num_decod_layers, dropout_rate, pad_index, bidirectional=False):\n",
    "#         super().__init__()\n",
    "\n",
    "#         self.layer_type = layer_type.lower()\n",
    "#         self.bidirectional = bidirectional\n",
    "#         self.hidden_layers_size = hidden_layers_size\n",
    "#         self.num_layers = num_decod_layers\n",
    "\n",
    "#         # Embedding layer\n",
    "#         self.embed = nn.Embedding(input_size, emb_dim, padding_idx=pad_index)\n",
    "#         nn.init.uniform_(self.embed.weight, -0.1, 0.1)\n",
    "\n",
    "#         # RNN type selection\n",
    "#         rnn_cls = self.layer_mode(self.layer_type)\n",
    "#         self.layer = rnn_cls(\n",
    "#             input_size=emb_dim,\n",
    "#             hidden_size=hidden_layers_size,\n",
    "#             num_layers=num_decod_layers,\n",
    "#             batch_first=True,\n",
    "#             bidirectional=bidirectional,\n",
    "#             dropout=dropout_rate if num_decod_layers > 1 else 0\n",
    "#         )\n",
    "\n",
    "#         # # Weight initialization for RNN\n",
    "#         # for name, param in self.layer.named_parameters():\n",
    "#         #     if 'weight' in name:\n",
    "#         #         nn.init.orthogonal_(param)\n",
    "#         #     elif 'bias' in name:\n",
    "#         #         nn.init.constant_(param, 0)\n",
    "\n",
    "#         # Linear layer\n",
    "#         rnn_output_dim = hidden_layers_size * (2 if bidirectional else 1)\n",
    "#         self.fc = nn.Linear(rnn_output_dim, output_size)\n",
    "\n",
    "#     def layer_mode(self, layer_type):\n",
    "#         if layer_type == \"rnn\":\n",
    "#             return nn.RNN\n",
    "#         elif layer_type == \"lstm\":\n",
    "#             return nn.LSTM\n",
    "#         else:\n",
    "#             return nn.GRU\n",
    "\n",
    "#     def forward(self, inputs: torch.LongTensor, hidden, cell=None):\n",
    "#         # Sanity check for input range\n",
    "#         if torch.any(inputs >= self.embed.num_embeddings):\n",
    "#             raise ValueError(f\"Input index out of range: max={inputs.max().item()}, vocab={self.embed.num_embeddings}\")\n",
    "\n",
    "#         inputs = inputs.unsqueeze(1)  # [batch_size, 1]\n",
    "\n",
    "#         # Embedding\n",
    "#         embed = self.embed(inputs)  # [batch_size, 1, emb_dim]\n",
    "#         if torch.isnan(embed).any() or torch.isinf(embed).any():\n",
    "#             raise AssertionError(\"NaN or Inf detected in decoder embedding\")\n",
    "\n",
    "#         # RNN computation\n",
    "#         try:\n",
    "#             if self.layer_type == \"lstm\":\n",
    "#                 outputs, (hidden, cell) = self.layer(embed, (hidden, cell))\n",
    "#             else:\n",
    "#                 outputs, hidden = self.layer(embed, hidden)\n",
    "#                 cell = None\n",
    "#         except Exception as e:\n",
    "#             print(f\"Decoder RNN Exception: {e}\")\n",
    "#             print(f\"Embed shape: {embed.shape}\")\n",
    "#             print(f\"Hidden shape: {hidden.shape}\")\n",
    "#             raise\n",
    "\n",
    "#         # Check RNN outputs\n",
    "#         if torch.isnan(outputs).any() or torch.isinf(outputs).any():\n",
    "#             print(\"NaN/Inf in decoder RNN output\", outputs)\n",
    "#             raise AssertionError(\"NaN or Inf in decoder RNN output\")\n",
    "\n",
    "#         # outputs = torch.clamp(outputs, -1e4, 1e4)  # clamp before linear layer\n",
    "\n",
    "#         # Final linear projection\n",
    "#         predict_word = self.fc(outputs.squeeze(1))  # [batch_size, output_vocab_size]\n",
    "#         if torch.isnan(predict_word).any() or torch.isinf(predict_word).any():\n",
    "#             raise AssertionError(\"NaN or Inf in decoder's predicted word logits\")\n",
    "\n",
    "#         return predict_word, hidden, cell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "af4274ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, layer_type, emb_dim, hidden_layers_size, num_encod_layers, dropout_rate, pad_index, bidirectional=False):\n",
    "        super().__init__()\n",
    "        self.layer_type = layer_type\n",
    "        self.layers = self.layer_mode(layer_type)\n",
    "        self.bidirectional = bidirectional  # Store bidirectional flag\n",
    "        self.num_encod_layers = num_encod_layers\n",
    "\n",
    "        self.embed = nn.Embedding(input_size, emb_dim, padding_idx=pad_index)\n",
    "        self.layer = self.layers(\n",
    "            emb_dim, \n",
    "            hidden_layers_size, \n",
    "            num_encod_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=bidirectional,\n",
    "            dropout=dropout_rate if num_encod_layers > 1 else 0\n",
    "        )\n",
    "\n",
    "    def layer_mode(self, layer_type):\n",
    "        layer_type = layer_type.lower()\n",
    "        if layer_type == \"rnn\":\n",
    "            return nn.RNN\n",
    "        elif layer_type == \"lstm\":\n",
    "            return nn.LSTM\n",
    "        else:\n",
    "            return nn.GRU\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        embed = self.embed(input_seq)\n",
    "        if self.layer_type == \"lstm\":\n",
    "            outputs, (hidden, cell) = self.layer(embed)\n",
    "        else:\n",
    "            outputs, hidden = self.layer(embed)\n",
    "            cell = None\n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "66af830d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size, output_size, layer_type, emb_dim, hidden_layers_size,\n",
    "                 num_decod_layers, dropout_rate, pad_index, bidirectional=False):\n",
    "        super().__init__()\n",
    "        self.layer_type = layer_type.lower()\n",
    "        self.bidirectional = bidirectional  # Store bidirectional flag\n",
    "        self.num_layers = num_decod_layers\n",
    "\n",
    "        self.embed = nn.Embedding(input_size, emb_dim, padding_idx=pad_index)\n",
    "        nn.init.uniform_(self.embed.weight, -0.1, 0.1)\n",
    "\n",
    "        rnn_cls = self.layer_mode(self.layer_type)\n",
    "        self.layer = rnn_cls(\n",
    "            emb_dim,\n",
    "            hidden_layers_size,\n",
    "            num_decod_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=bidirectional,\n",
    "            dropout=dropout_rate if num_decod_layers > 1 else 0\n",
    "        )\n",
    "\n",
    "        rnn_output_dim = hidden_layers_size * (2 if bidirectional else 1)\n",
    "        self.fc = nn.Linear(rnn_output_dim, output_size)\n",
    "\n",
    "    def layer_mode(self, layer_type):\n",
    "        if layer_type == \"rnn\":\n",
    "            return nn.RNN\n",
    "        elif layer_type == \"lstm\":\n",
    "            return nn.LSTM\n",
    "        else:\n",
    "            return nn.GRU\n",
    "\n",
    "    def forward(self, inputs: torch.LongTensor, hidden, cell=None):\n",
    "        inputs = inputs.unsqueeze(1)\n",
    "        embed = self.embed(inputs)\n",
    "        \n",
    "        if self.layer_type == \"lstm\":\n",
    "            outputs, (hidden, cell) = self.layer(embed, (hidden, cell))\n",
    "        else:\n",
    "            outputs, hidden = self.layer(embed, hidden)\n",
    "            cell = None\n",
    "            \n",
    "        predict_word = self.fc(outputs.squeeze(1))\n",
    "        return predict_word, hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "c6bb2574",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Sequence2Sequence(nn.Module):\n",
    "    def __init__(self, encoder, decoder, output_vocab):\n",
    "        super().__init__()\n",
    "        self.output_vocab = output_vocab\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.layer_type = encoder.layer_type.lower()\n",
    "\n",
    "    def adjust_hidden(self, hidden, desired_layers):\n",
    "        current_layers = hidden.size(0)\n",
    "        if current_layers < desired_layers:\n",
    "            zeros = torch.zeros(desired_layers - current_layers, \n",
    "                              hidden.size(1), \n",
    "                              hidden.size(2),\n",
    "                              device=hidden.device,\n",
    "                              dtype=hidden.dtype)\n",
    "            adjusted = torch.cat([hidden, zeros], dim=0)\n",
    "        else:\n",
    "            adjusted = hidden[:desired_layers]\n",
    "        return adjusted\n",
    "\n",
    "    def forward(self, input_sequence, target_sequence, teacher_force_ratio=0.5):\n",
    "        batch_size = input_sequence.size(0)\n",
    "        target_len = target_sequence.size(1)\n",
    "        outputs = torch.zeros(batch_size, target_len, len(self.output_vocab)).to(input_sequence.device)\n",
    "\n",
    "        hidden, cell = self.encoder(input_sequence)\n",
    "        \n",
    "        # Calculate required dimensions\n",
    "        encoder_directions = 2 if self.encoder.bidirectional else 1\n",
    "        decoder_directions = 2 if self.decoder.bidirectional else 1\n",
    "        encoder_total = self.encoder.num_encod_layers * encoder_directions\n",
    "        decoder_total = self.decoder.num_layers * decoder_directions\n",
    "\n",
    "        # Adjust hidden states\n",
    "        hidden = self.adjust_hidden(hidden, decoder_total)\n",
    "        cell = self.adjust_hidden(cell, decoder_total) if cell is not None else None\n",
    "\n",
    "        x = target_sequence[:, 0]\n",
    "        for t in range(1, target_len):\n",
    "            output, hidden, cell = self.decoder(x, hidden, cell)\n",
    "            outputs[:, t] = output\n",
    "            x = target_sequence[:, t] if random.random() < teacher_force_ratio else output.argmax(1)\n",
    "            \n",
    "        return outputs\n",
    "\n",
    "    def beam_search_decode(self, input_sequence, sos_token, eos_token, beam_width=3, max_len=30):\n",
    "        self.encoder.eval()\n",
    "        self.decoder.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            device = input_sequence.device\n",
    "            hidden, cell = self.encoder(input_sequence)\n",
    "\n",
    "            # Calculate required dimensions\n",
    "            encoder_directions = 2 if self.encoder.bidirectional else 1\n",
    "            decoder_directions = 2 if self.decoder.bidirectional else 1\n",
    "            encoder_total = self.encoder.num_encod_layers * encoder_directions\n",
    "            decoder_total = self.decoder.num_layers * decoder_directions\n",
    "\n",
    "            # Adjust hidden states\n",
    "            hidden = self.adjust_hidden(hidden, decoder_total)\n",
    "            cell = self.adjust_hidden(cell, decoder_total) if cell is not None else None\n",
    "\n",
    "            beams = [([sos_token], 0.0, hidden, cell)]\n",
    "            completed_sequences = []\n",
    "\n",
    "            for _ in range(max_len):\n",
    "                temp_beams = []\n",
    "                for seq, score, h, c in beams:\n",
    "                    if seq[-1] == eos_token:\n",
    "                        completed_sequences.append((seq, score))\n",
    "                        continue\n",
    "\n",
    "                    last_token = torch.LongTensor([seq[-1]]).to(device)\n",
    "                    out, h_new, c_new = self.decoder(last_token, h, c)\n",
    "                    log_probs = torch.log_softmax(out, dim=1)\n",
    "                    top_log_probs, top_indices = torch.topk(log_probs, beam_width)\n",
    "\n",
    "                    for i in range(beam_width):\n",
    "                        token = top_indices[0][i].item()\n",
    "                        new_seq = seq + [token]\n",
    "                        new_score = score + top_log_probs[0][i].item()\n",
    "                        temp_beams.append((new_seq, new_score, h_new, c_new))\n",
    "\n",
    "                beams = sorted(temp_beams, key=lambda x: x[1], reverse=True)[:beam_width]\n",
    "                if all(seq[-1] == eos_token for seq, _, _, _ in beams):\n",
    "                    completed_sequences.extend(beams)\n",
    "                    break\n",
    "\n",
    "            if not completed_sequences:\n",
    "                completed_sequences = beams\n",
    "\n",
    "            best_sequence = max(completed_sequences, key=lambda x: x[1])[0]\n",
    "            return best_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "3f8b9c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Sequence2Sequence(nn.Module):\n",
    "#     def __init__(self, encoder, decoder, output_vocab):\n",
    "#         super().__init__()\n",
    "#         self.output_vocab = output_vocab\n",
    "#         self.encoder = encoder\n",
    "#         self.decoder = decoder\n",
    "#         self.layer_type = encoder.layer_type.lower()\n",
    "\n",
    "#     def forward(self, input_sequence, target_sequence, teacher_force_ratio=0.5):\n",
    "#         batch_size = input_sequence.size(0)\n",
    "#         target_len = target_sequence.size(1)\n",
    "#         target_vocab_size = len(self.output_vocab)\n",
    "\n",
    "#         outputs = torch.zeros(batch_size, target_len, target_vocab_size).to(input_sequence.device)\n",
    "\n",
    "#         # Get encoder outputs\n",
    "#         if self.layer_type == \"lstm\":\n",
    "#             hidden, cell = self.encoder(input_sequence)  # hidden: (num_layers, batch, hidden_size)\n",
    "#         else:\n",
    "#             hidden, cell = self.encoder(input_sequence)\n",
    "#             cell = None  # not used for GRU/RNN\n",
    "\n",
    "#         # Decoder input starts with <sos>\n",
    "#         x = target_sequence[:, 0]\n",
    "\n",
    "#         for t in range(1, target_len):\n",
    "#             output, hidden, cell = self.decoder(x, hidden, cell)\n",
    "#             outputs[:, t] = output\n",
    "\n",
    "#             best_guess = output.argmax(1)\n",
    "#             # best_guess = torch.clamp(best_guess, min=0, max=target_vocab_size - 1)\n",
    "#             x = target_sequence[:, t] if random.random() < teacher_force_ratio else best_guess\n",
    "\n",
    "#         return outputs\n",
    "    \n",
    "#     def beam_search_decode(self, input_sequence, sos_token, eos_token, beam_width=3, max_len=30):\n",
    "#         \"\"\"\n",
    "#         Beam Search for decoding (only supports batch_size=1 for now).\n",
    "#         \"\"\"\n",
    "#         self.encoder.eval()\n",
    "#         self.decoder.eval()\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             assert input_sequence.size(0) == 1, \"Beam search currently supports batch_size = 1 only\"\n",
    "#             device = input_sequence.device\n",
    "\n",
    "#             # Encode input\n",
    "#             if self.layer_type == \"lstm\":\n",
    "#                 hidden, cell = self.encoder(input_sequence)\n",
    "#             else:\n",
    "#                 hidden, cell = self.encoder(input_sequence)\n",
    "#                 cell = None\n",
    "\n",
    "#             # Initialize beams: (sequence, cumulative log prob, hidden, cell)\n",
    "#             beams = [([sos_token], 0.0, hidden, cell)]\n",
    "#             completed_sequences = []\n",
    "\n",
    "#             for _ in range(max_len):\n",
    "#                 temp_beams = []\n",
    "\n",
    "#                 for seq, score, h, c in beams:\n",
    "#                     if seq[-1] == eos_token:\n",
    "#                         # Already ended sequence, just carry forward\n",
    "#                         completed_sequences.append((seq, score))\n",
    "#                         continue\n",
    "\n",
    "#                     last_token = torch.LongTensor([seq[-1]]).to(device)\n",
    "\n",
    "#                     out, h_new, c_new = self.decoder(last_token, h, c)\n",
    "#                     log_probs = torch.log_softmax(out, dim=1)\n",
    "\n",
    "#                     top_log_probs, top_indices = torch.topk(log_probs, beam_width, dim=1)\n",
    "\n",
    "#                     for i in range(beam_width):\n",
    "#                         token = top_indices[0][i].item()\n",
    "#                         token_score = top_log_probs[0][i].item()\n",
    "#                         new_seq = seq + [token]\n",
    "#                         new_score = score + token_score\n",
    "\n",
    "#                         temp_beams.append((new_seq, new_score, h_new, c_new))\n",
    "\n",
    "#                 # Prune beams to keep top-k\n",
    "#                 beams = sorted(temp_beams, key=lambda x: x[1], reverse=True)[:beam_width]\n",
    "\n",
    "#                 # If all beams end with <eos>, we can stop early\n",
    "#                 if all(seq[-1] == eos_token for seq, _, _, _ in beams):\n",
    "#                     completed_sequences.extend((seq, score) for seq, score, _, _ in beams)\n",
    "#                     break\n",
    "\n",
    "#             if not completed_sequences:\n",
    "#                 completed_sequences = [(seq, score) for seq, score, _, _ in beams]\n",
    "\n",
    "#             # Select best sequence by log-prob\n",
    "#             best_sequence = sorted(completed_sequences, key=lambda x: x[1], reverse=True)[0][0]\n",
    "#             return best_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "2e8723d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # Assuming the Encoder output from earlier\n",
    "# encoder = Encoder(\n",
    "#     input_size=len(vocab_in),\n",
    "#     layer_type= \"gru\",\n",
    "#     emb_dim=200,\n",
    "#     hidden_layers_size=512,\n",
    "#     num_encod_layers=1,\n",
    "#     dropout_rate=0.0\n",
    "#     , pad_index=vocab_in[PAD],\n",
    "#     bidirectional=False\n",
    "# )\n",
    "\n",
    "# # Initialize the Decoder\n",
    "# decoder = Decoder(\n",
    "#     input_size=len(vocab_out),  # output vocabulary size\n",
    "#     output_size=len(vocab_out),  # output vocabulary size\n",
    "#     layer_type= \"gru\",\n",
    "#     emb_dim=200,\n",
    "#     hidden_layers_size=512,\n",
    "#     num_decod_layers=1,\n",
    "#     dropout_rate=0.0\n",
    "#     , pad_index=vocab_out[PAD],\n",
    "#     bidirectional=False\n",
    "# )\n",
    "\n",
    "# learning_rate = 0.001\n",
    "\n",
    "# device = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "# seq2seq = Sequence2Sequence(encoder, decoder, vocab_out).to(device)\n",
    "\n",
    "# optimizer = optim.Adam(seq2seq.parameters(), lr=learning_rate)\n",
    "# criterion = nn.CrossEntropyLoss(ignore_index=output_vocab[PAD]).to(device)\n",
    "\n",
    "\n",
    "\n",
    "# # Check the range of vocabulary indices in vocab_out\n",
    "# # print(\"Vocab output indices:\", list(vocab_out.values()))\n",
    "# # print(\"Vocab size:\", len(vocab_out))\n",
    "\n",
    "# for i in range(5):      \n",
    "    \n",
    "\n",
    "\n",
    "#     for input_batch, target_batch in train_dataloader:\n",
    "#         target_batch[target_batch >= len(vocab_out)] = vocab_out[UNK]\n",
    "        \n",
    "        \n",
    "#         # print(\"Target batch shape in train method\", target_batch.shape)\n",
    "\n",
    "#         # print(\"Total batches in train method\", len(self.dataloader))\n",
    "\n",
    "#         input_batch = input_batch.to(device)\n",
    "#         target_batch = target_batch.to(device)\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         output = seq2seq(input_batch, target_batch, teacher_force_ratio=0.5)\n",
    "#         # print(\"Output shape in train method\", output.shape)\n",
    "\n",
    "#         _ , predicted = torch.max(output, dim=2)\n",
    "\n",
    "#         # Ignore <sos> token when evaluating\n",
    "#         predicted_trimmed = predicted[:, 1:]\n",
    "#         target_trimmed = target_batch[:, 1:]\n",
    "\n",
    "#         # Flatten the output and target for loss calculation\n",
    "#         output_flatten = output.view(-1, output.shape[-1])\n",
    "#         # print(\"Flattened output shape in train method\", output_flatten.shape)\n",
    "\n",
    "#         target_flatten = target_batch.reshape(-1).to(device)\n",
    "#         # print(\"Flattened target shape in train method\", target_flatten.shape)\n",
    "\n",
    "#         loss = criterion(output_flatten, target_flatten)\n",
    "#         if torch.isnan(loss) or torch.isinf(loss):\n",
    "#             raise ValueError(\"Loss exploded to NaN or Inf\")\n",
    "\n",
    "#         print(\"Loss:\", loss.item())  # during debugging\n",
    "#         # print(\"Sequence Accuracy:\", acc['sequence_accuracy'])\n",
    "#         # print(\"Character Accuracy:\", acc['character_accuracy'])\n",
    "\n",
    "#         loss.backward()\n",
    "#         # Add gradient clipping here\n",
    "#         torch.nn.utils.clip_grad_norm_(seq2seq.parameters(), max_norm=1)\n",
    "#         optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "6e2d0f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AccuracyCalculator:\n",
    "    def __init__(self, eos_token: str, pad_token: str, vocab_out: dict, device: torch.device):\n",
    "        \"\"\"\n",
    "        eos_token: the string for <eos>\n",
    "        pad_token: the string for <pad>\n",
    "        vocab_out:  token->index mapping for your output vocab\n",
    "        device:     torch.device (e.g. 'cuda' or 'cpu')\n",
    "        \"\"\"\n",
    "        self.device = device\n",
    "        self.eos_idx = vocab_out[eos_token]\n",
    "        self.pad_idx = vocab_out[pad_token]\n",
    "\n",
    "\n",
    "    def _trim_batch_at_eos(self, sequences: torch.LongTensor):\n",
    "        \"\"\"\n",
    "        sequences: (batch_size, seq_len)\n",
    "        Returns: list of 1D LongTensors, each trimmed to include its first <eos> (if any),\n",
    "                 or the full length if no <eos> appears.\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = sequences.size()\n",
    "        trimmed = []\n",
    "        # move to CPU & numpy for easy indexing\n",
    "        seqs = sequences.detach().cpu().tolist()\n",
    "        for seq in seqs:\n",
    "            if self.eos_idx in seq:\n",
    "                end = seq.index(self.eos_idx) + 1\n",
    "                trimmed.append(torch.tensor(seq[:end], dtype=torch.long, device=self.device))\n",
    "            else:\n",
    "                trimmed.append(torch.tensor(seq, dtype=torch.long, device=self.device))\n",
    "        return trimmed\n",
    "\n",
    "    def compute_accuracy(self,\n",
    "                         predictions: torch.LongTensor,\n",
    "                         targets:     torch.LongTensor\n",
    "                         ) -> dict:\n",
    "        \"\"\"\n",
    "        predictions: (batch_size, seq_len) of token-indices, already argmaxed\n",
    "        targets:     (batch_size, seq_len) of token-indices, contains <sos>â€¦<eos> and padding\n",
    "        \"\"\"\n",
    "        predictions = predictions.to(self.device)\n",
    "        targets     = targets.to(self.device)\n",
    "\n",
    "        batch_size, seq_len = targets.shape\n",
    "\n",
    "        # 1) Character-level accuracy (ignoring PAD completely)\n",
    "        nonpad_mask   = targets != self.pad_idx                # (B, L) bool\n",
    "        char_correct  = ((predictions == targets) & nonpad_mask).sum().item()\n",
    "        char_total    = nonpad_mask.sum().item()\n",
    "        char_accuracy = char_correct / char_total if char_total > 0 else 0.0\n",
    "\n",
    "        # 2) Sequence-level accuracy\n",
    "        #    Trim both preds & targets at each target's <eos>, then compare exactly.\n",
    "        pred_trimmed = self._trim_batch_at_eos(predictions)\n",
    "        targ_trimmed = self._trim_batch_at_eos(targets)\n",
    "\n",
    "        seq_correct = 0\n",
    "        for p_seq, t_seq in zip(pred_trimmed, targ_trimmed):\n",
    "            if p_seq.size(0) == t_seq.size(0) and torch.equal(p_seq, t_seq):\n",
    "                seq_correct += 1\n",
    "\n",
    "        seq_accuracy = seq_correct / batch_size if batch_size > 0 else 0.0\n",
    "\n",
    "        return {\n",
    "            'sequence_accuracy':   seq_accuracy,\n",
    "            'character_accuracy':  char_accuracy,\n",
    "            'correct_sequences':   seq_correct,\n",
    "            'total_sequences':     batch_size,\n",
    "            'correct_characters':  char_correct,\n",
    "            'total_characters':    char_total\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "7423b7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Train_Model:\n",
    "    def __init__(self, seq2seq, dataloader, optimizer, loss_fn, acc_calculator, device):\n",
    "        self.dataloader = dataloader\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "        self.acc_calculator = acc_calculator\n",
    "        self.device = device\n",
    "        self.seq2seq = seq2seq\n",
    "\n",
    "    def count_params(self,model):\n",
    "        return sum(param.numel() for param in model.parameters() if param.requires_grad)\n",
    "\n",
    "    def train(self, teacher_force_ratio=0):\n",
    "        self.seq2seq.to(self.device)\n",
    "        self.seq2seq.train()\n",
    "\n",
    "        epoch_loss = 0\n",
    "        seq_acc = 0\n",
    "        character_acc = 0\n",
    "        total_seqs = 0\n",
    "        total_chars = 0\n",
    "\n",
    "        progress_bar = tqdm(self.dataloader, desc=\"Training Batches\")\n",
    "\n",
    "\n",
    "        for input_batch, target_batch in progress_bar:\n",
    "            \n",
    "            # print(\"Target batch shape in train method\", target_batch.shape)\n",
    "\n",
    "            # print(\"Total batches in train method\", len(self.dataloader))\n",
    "\n",
    "            input_batch = input_batch.to(self.device)\n",
    "            target_batch = target_batch.to(self.device)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            output = self.seq2seq(input_batch, target_batch, teacher_force_ratio)\n",
    "            # print(\"Output shape in train method\", output.shape)\n",
    "\n",
    "            _ , predicted = torch.max(output, dim=2)\n",
    "\n",
    "            # Ignore <sos> token when evaluating\n",
    "            predicted_trimmed = predicted[:, 1:]\n",
    "            target_trimmed = target_batch[:, 1:]\n",
    "\n",
    "            # Flatten the output and target for loss calculation\n",
    "            output_flatten = output.view(-1, output.shape[-1])\n",
    "            # print(\"Flattened output shape in train method\", output_flatten.shape)\n",
    "\n",
    "            target_flatten = target_batch.reshape(-1).to(self.device)\n",
    "            # print(\"Flattened target shape in train method\", target_flatten.shape)\n",
    "\n",
    "            loss = self.loss_fn(output_flatten, target_flatten)\n",
    "            acc = self.acc_calculator.compute_accuracy(predicted_trimmed, target_trimmed)\n",
    "            # print(\"Sequence Accuracy:\", acc['sequence_accuracy'])\n",
    "            # print(\"Character Accuracy:\", acc['character_accuracy'])\n",
    "\n",
    "            loss.backward()\n",
    "            # Add gradient clipping here\n",
    "            torch.nn.utils.clip_grad_norm_(self.seq2seq.parameters(), max_norm=1)\n",
    "            self.optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            seq_acc += acc['correct_sequences']\n",
    "            total_seqs += acc['total_sequences']\n",
    "\n",
    "            character_acc += acc['correct_characters']\n",
    "            total_chars += acc['total_characters']\n",
    "\n",
    "            avg_seq_acc = seq_acc / total_seqs if total_seqs > 0 else 0.0\n",
    "            avg_char_acc = character_acc / total_chars if total_chars > 0 else 0.0\n",
    "\n",
    "            progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "        return epoch_loss / len(self.dataloader), avg_char_acc, avg_seq_acc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "54a971ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluate_Model:\n",
    "    def __init__(self, seq2seq, dataloader, loss_fn, acc_calculator, device):\n",
    "        self.dataloader = dataloader\n",
    "        self.loss_fn = loss_fn\n",
    "        self.acc_calculator = acc_calculator\n",
    "        self.device = device\n",
    "        self.seq2seq = seq2seq\n",
    "\n",
    "    def evaluate(self, beam_search=False, beam_width=3):\n",
    "        self.seq2seq.eval()\n",
    "        self.seq2seq.to(self.device)\n",
    "\n",
    "        epoch_loss = 0\n",
    "        seq_acc = 0\n",
    "        character_acc = 0\n",
    "        total_seqs = 0\n",
    "        total_chars = 0\n",
    "\n",
    "        progress_bar = tqdm(self.dataloader, desc=\"Evaluation Batches\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for input_batch, target_batch in progress_bar:\n",
    "                input_batch = input_batch.to(self.device)\n",
    "                target_batch = target_batch.to(self.device)\n",
    "                \n",
    "\n",
    "                if beam_search:\n",
    "                    # Beam search decoding\n",
    "                    batch_size = input_batch.size(0)\n",
    "                    decoded_batch = []\n",
    "                    for i in range(batch_size):\n",
    "                        predicted_ids = self.seq2seq.beam_search_decode(\n",
    "                            input_batch[i].unsqueeze(0),\n",
    "                            sos_token=self.seq2seq.output_vocab[SOS],\n",
    "                            eos_token=self.seq2seq.output_vocab[EOS],\n",
    "                            beam_width=beam_width\n",
    "                        )\n",
    "                        decoded_batch.append(torch.tensor(predicted_ids, device=self.device))\n",
    "\n",
    "                    # Pad decoded sequences for comparison\n",
    "                    max_len = max(len(seq) for seq in decoded_batch)\n",
    "                    predicted_tensor = torch.full((batch_size, max_len), fill_value=self.seq2seq.output_vocab[PAD], device=self.device)\n",
    "                    for i, seq in enumerate(decoded_batch):\n",
    "                        predicted_tensor[i, :len(seq)] = seq\n",
    "                else:\n",
    "                    output = self.seq2seq(input_batch, target_batch, teacher_force_ratio=0)\n",
    "                    _, predicted = torch.max(output, dim=2)\n",
    "                    predicted_tensor = predicted\n",
    "\n",
    "                # --- Fix for length mismatch between prediction and target ---\n",
    "                predicted_tensor = predicted_tensor[:, :target_batch.size(1)]\n",
    "                if predicted_tensor.size(1) < target_batch.size(1):\n",
    "                    pad_len = target_batch.size(1) - predicted_tensor.size(1)\n",
    "                    pad = torch.full((predicted_tensor.size(0), pad_len), self.seq2seq.output_vocab[PAD], device=self.device)\n",
    "                    predicted_tensor = torch.cat([predicted_tensor, pad], dim=1)\n",
    "                # -------------------------------------------------------------\n",
    "\n",
    "                # Compute loss\n",
    "                output = self.seq2seq(input_batch, target_batch, teacher_force_ratio=0)\n",
    "                output_flat = output.view(-1, output.shape[-1])\n",
    "                target_flat = target_batch.view(-1)\n",
    "\n",
    "                loss = self.loss_fn(output_flat, target_flat)\n",
    "\n",
    "                # Accuracy\n",
    "                pred_trimmed = predicted_tensor[:, 1:]\n",
    "                target_trimmed = target_batch[:, 1:]\n",
    "\n",
    "                acc = self.acc_calculator.compute_accuracy(pred_trimmed, target_trimmed)\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "                seq_acc += acc['correct_sequences']\n",
    "                total_seqs += acc['total_sequences']\n",
    "                character_acc += acc['correct_characters']\n",
    "                total_chars += acc['total_characters']\n",
    "\n",
    "                avg_seq_acc = seq_acc / total_seqs if total_seqs > 0 else 0.0\n",
    "                avg_char_acc = character_acc / total_chars if total_chars > 0 else 0.0\n",
    "                progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "        return epoch_loss / len(self.dataloader), avg_char_acc, avg_seq_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "730c4b7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS device\n"
     ]
    }
   ],
   "source": [
    "# Check if mps is available\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS device\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU device\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "a14c2f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "75470420",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Build_Model:\n",
    "    def __init__(self, \n",
    "                 sequence_data_preprocessor,\n",
    "                 encoder, \n",
    "                 decoder, \n",
    "                 seq2seq, \n",
    "                 batch_size,\n",
    "                 train_path,\n",
    "                 val_path,\n",
    "                 device = device\n",
    "                 ):\n",
    "        \n",
    "        self.sequence_data_preprocessor = sequence_data_preprocessor\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.seq2seq = seq2seq\n",
    "        self.batch_size = batch_size\n",
    "        self.train_path = train_path\n",
    "        self.val_path = val_path\n",
    "        self.device = device\n",
    "\n",
    "        # Process datasets\n",
    "        train_processor = self.sequence_data_preprocessor(self.train_path)\n",
    "        self.train_input_tensor, self.train_target_tensor, self.input_vocab , self.output_vocab = train_processor.prepare_tensors()\n",
    "\n",
    "        # Pass shared vocab to val/test processors\n",
    "        val_processor = self.sequence_data_preprocessor(self.val_path, input_vocab=self.input_vocab, output_vocab=self.output_vocab)\n",
    "        self.val_input_tensor, self.val_target_tensor, _, _ = val_processor.prepare_tensors()\n",
    "\n",
    "\n",
    "\n",
    "        # Creat Datasets\n",
    "        train_data = Datasets(self.train_input_tensor, self.train_target_tensor)\n",
    "        val_data = Datasets(self.val_input_tensor, self.val_target_tensor)\n",
    "\n",
    "\n",
    "        # Create DataLoader\n",
    "        self.train_dataloader = DataLoader(train_data, batch_size=self.batch_size, shuffle=True)\n",
    "        self.val_dataloader = DataLoader(val_data, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "    def epoch_time(self, start_time, end_time):\n",
    "        e_time = end_time - start_time\n",
    "        mins = e_time // 60\n",
    "        secs = e_time%60\n",
    "        return mins, secs,\n",
    "\n",
    "    def build(\n",
    "        self, emb_size, layer_type, hidden_layers_size, num_encod_layers, num_decod_layers,\n",
    "        dropout_rate, epochs, learning_rate, teacher_force_ratio=0, bidirectional=False,\n",
    "        save_path='best_model.pt', patience=3, val_beam_search=False, beam_width=3,\n",
    "        testing_phase=False, test_path=None, test_beam_search=False\n",
    "    ):\n",
    "        # Instantiate encoder, decoder, seq2seq as before\n",
    "        encoder = self.encoder(\n",
    "            input_size=len(self.input_vocab),\n",
    "            layer_type=layer_type,\n",
    "            emb_dim=emb_size,\n",
    "            hidden_layers_size=hidden_layers_size,\n",
    "            num_encod_layers=num_encod_layers,\n",
    "            dropout_rate=dropout_rate,\n",
    "            pad_index=self.input_vocab[PAD],\n",
    "            bidirectional=bidirectional\n",
    "        ).to(self.device)\n",
    "\n",
    "        decoder = self.decoder(\n",
    "            input_size=len(self.output_vocab),\n",
    "            output_size=len(self.output_vocab),\n",
    "            layer_type=layer_type,\n",
    "            emb_dim=emb_size,\n",
    "            hidden_layers_size=hidden_layers_size,\n",
    "            num_decod_layers=num_decod_layers,\n",
    "            dropout_rate=dropout_rate,\n",
    "            pad_index=self.output_vocab[PAD],\n",
    "            bidirectional=bidirectional\n",
    "        ).to(self.device)\n",
    "\n",
    "        seq2seq = self.seq2seq(encoder, decoder, self.output_vocab).to(self.device)\n",
    "\n",
    "        optimizer = optim.Adam(seq2seq.parameters(), lr=learning_rate)\n",
    "        criterion = nn.CrossEntropyLoss(ignore_index=self.output_vocab[PAD]).to(self.device)\n",
    "\n",
    "        acc_calculator = AccuracyCalculator(\n",
    "            eos_token=EOS,\n",
    "            pad_token=PAD,\n",
    "            vocab_out=self.output_vocab,\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "        train_model = Train_Model(seq2seq, self.train_dataloader, optimizer, criterion, acc_calculator, self.device)\n",
    "        evaluate_model = Evaluate_Model(seq2seq, self.val_dataloader, criterion, acc_calculator, self.device)\n",
    "\n",
    "        print(f'The model has {train_model.count_params(seq2seq):,} trainable parameters')\n",
    "\n",
    "        best_val_loss = float('inf')\n",
    "        best_model_state = None\n",
    "        epochs_no_improve = 0\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            start_time = time.time()\n",
    "\n",
    "            # === Compute teacher forcing ratio (Exponential Decay) ===\n",
    "            decay_rate = 0.05  # You can tune this (0.03 - 0.1 are typical)\n",
    "            current_tfr = teacher_force_ratio * np.exp(-decay_rate * epoch)\n",
    "            current_tfr = max(0.0, current_tfr)  # Prevent going below 0\n",
    "            print(f'\\nEpoch {epoch+1}/{epochs}{\" \"*40}Teacher Forcing Ratio: {current_tfr:.4f}\\n{\"-\"*80}')\n",
    "\n",
    "            train_loss, train_char_acc, train_seq_acc = train_model.train(teacher_force_ratio=current_tfr)\n",
    "\n",
    "            print(f'Train Loss: {train_loss:.4f} | Train Char Acc: {train_char_acc:.4f} | Train Seq Acc: {train_seq_acc:.4f}')\n",
    "\n",
    "\n",
    "            val_loss, val_char_acc, val_seq_acc = evaluate_model.evaluate(beam_search=val_beam_search, beam_width=beam_width)\n",
    "            print(f'Val   Loss: {val_loss:.4f} | Val Char Acc: {val_char_acc:.4f} | Val Seq Acc: {val_seq_acc:.4f}')\n",
    "\n",
    "            end_time = time.time()\n",
    "            epoch_mins, epoch_secs = self.epoch_time(start_time, end_time)\n",
    "\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_model_state = copy.deepcopy(seq2seq.state_dict())\n",
    "                epochs_no_improve = 0\n",
    "                print(f\"Validation improved, but waiting to confirm best over next {patience} epochs...\")\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "                print(f\" No improvement. Patience: {epochs_no_improve}/{patience}\")\n",
    "\n",
    "                if epochs_no_improve >= patience:\n",
    "                    print(f\"\\n Saving best model from {epoch+1 - patience} epoch(s) ago with val loss: {best_val_loss:.4f}\")\n",
    "                    torch.save(best_model_state, save_path)\n",
    "                    print(f\" Best model saved to: {save_path}\")\n",
    "                    break\n",
    "\n",
    "            print(f'\\nEpoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "\n",
    "        if best_model_state is not None:\n",
    "            print(f'\\nTraining ended before confirming best model due to patience.\\n\\n{\"+\"*24}<Training Ended after {epochs} Epochs>{\"+\"*24}')\n",
    "            torch.save(best_model_state, save_path)\n",
    "        else:\n",
    "            print(f'\\nNo improvement observed. Saving final model.\\n\\n{\"+\"*24}<Training Ended after {epochs} Epochs>{\"+\"*24}')\n",
    "            torch.save(seq2seq.state_dict(), save_path)\n",
    "\n",
    "        # === TESTING PHASE BLOCK ===\n",
    "        if testing_phase:\n",
    "            test_path = test_path\n",
    "            if test_path is None:\n",
    "                raise ValueError(\"Test path must be provided for testing_phase=True.\")\n",
    "\n",
    "            print(f'\\n\\n\\n{\"+\"*28}<Testing Phase Started>{\"+\"*28}\\nPreparing test dataset...')\n",
    "            test_processor = self.sequence_data_preprocessor(test_path, input_vocab=self.input_vocab, output_vocab=self.output_vocab)\n",
    "            test_input_tensor, test_target_tensor, _, _ = test_processor.prepare_tensors()\n",
    "            test_data = Datasets(test_input_tensor, test_target_tensor)\n",
    "            self.test_dataloader = DataLoader(test_data, batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "            print(\"\\nLoading best model for test evaluation...\")\n",
    "            best_seq2seq = self.seq2seq(\n",
    "                encoder=self.encoder(\n",
    "                    input_size=len(self.input_vocab),\n",
    "                    layer_type=layer_type,\n",
    "                    emb_dim=emb_size,\n",
    "                    hidden_layers_size=hidden_layers_size,\n",
    "                    num_encod_layers=num_encod_layers,\n",
    "                    dropout_rate=dropout_rate,\n",
    "                    pad_index=self.input_vocab[PAD],\n",
    "                    bidirectional=bidirectional\n",
    "                ),\n",
    "                decoder=self.decoder(\n",
    "                    input_size=len(self.output_vocab),\n",
    "                    output_size=len(self.output_vocab),\n",
    "                    layer_type=layer_type,\n",
    "                    emb_dim=emb_size,\n",
    "                    hidden_layers_size=hidden_layers_size,\n",
    "                    num_decod_layers=num_decod_layers,\n",
    "                    dropout_rate=dropout_rate,\n",
    "                    pad_index=self.output_vocab[PAD],\n",
    "                    bidirectional=bidirectional\n",
    "                ),\n",
    "                output_vocab=self.output_vocab\n",
    "            ).to(self.device)\n",
    "\n",
    "            best_seq2seq.load_state_dict(torch.load(save_path))\n",
    "            best_seq2seq.eval()\n",
    "\n",
    "            test_eval_model = Evaluate_Model(\n",
    "                seq2seq=best_seq2seq,\n",
    "                dataloader=self.test_dataloader,\n",
    "                loss_fn=criterion,\n",
    "                acc_calculator=acc_calculator,\n",
    "                device=self.device\n",
    "            )\n",
    "\n",
    "            test_loss, test_char_acc, test_seq_acc = test_eval_model.evaluate(beam_search=test_beam_search, beam_width=beam_width)\n",
    "\n",
    "            print(f'\\n Test Loss: {test_loss:.4f} | Test Char Acc: {test_char_acc:.4f} | Test Seq Acc: {test_seq_acc:.4f}\\n{\"+\"*80}')\n",
    "        else:\n",
    "            print(f'\\n No test evaluation triggered. To evaluate, set `testing_phase=True`.\\n{\"+\"*80}')\n",
    "\n",
    "        # === RETURN ===\n",
    "        Loss_accuracy_log = {\n",
    "            'train_loss': train_loss,\n",
    "            'train_char_acc': train_char_acc,\n",
    "            'train_seq_acc': train_seq_acc,\n",
    "            'val_loss': val_loss,\n",
    "            'val_char_acc': val_char_acc,\n",
    "            'val_seq_acc': val_seq_acc\n",
    "        }\n",
    "\n",
    "        if testing_phase:\n",
    "            Loss_accuracy_log.update({\n",
    "                'test_loss': test_loss,\n",
    "                'test_char_acc': test_char_acc,\n",
    "                'test_seq_acc': test_seq_acc\n",
    "            })\n",
    "\n",
    "        Log_best_model_params = {\n",
    "            \"input_vocab\": self.input_vocab,\n",
    "            \"output_vocab\": self.output_vocab,\n",
    "            \"emb_size\": emb_size,\n",
    "            \"layer_type\": layer_type,\n",
    "            \"hidden_layers_size\": hidden_layers_size,\n",
    "            \"num_encod_layers\": num_encod_layers,\n",
    "            \"num_decod_layers\": num_decod_layers,\n",
    "            \"dropout_rate\": dropout_rate,\n",
    "            \"learning_rate\": learning_rate,\n",
    "            \"epochs\": epochs,\n",
    "            \"teacher_force_ratio\": teacher_force_ratio,\n",
    "            \"bidirectional\": bidirectional,\n",
    "            \"patience\": patience,\n",
    "            \"beam_width\": beam_width,\n",
    "            \"save_path\": save_path\n",
    "        }\n",
    "\n",
    "        return seq2seq, train_model, evaluate_model, Loss_accuracy_log, Log_best_model_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "12145ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 1,370,688 trainable parameters\n",
      "\n",
      "Epoch 1/5                                        Teacher Forcing Ratio: 0.5000\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2955/2955 [01:30<00:00, 32.82it/s, loss=2.8] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3.0364 | Train Char Acc: 0.2177 | Train Seq Acc: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 290/290 [00:22<00:00, 13.17it/s, loss=3.47]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val   Loss: 3.3168 | Val Char Acc: 0.1291 | Val Seq Acc: 0.0000\n",
      "Validation improved, but waiting to confirm best over next 3 epochs...\n",
      "\n",
      "Epoch Time: 1.0m 52.056195974349976s\n",
      "\n",
      "Epoch 2/5                                        Teacher Forcing Ratio: 0.4756\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2955/2955 [01:31<00:00, 32.37it/s, loss=2.78]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.8655 | Train Char Acc: 0.2631 | Train Seq Acc: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 290/290 [00:22<00:00, 12.94it/s, loss=3.43]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val   Loss: 3.3496 | Val Char Acc: 0.1241 | Val Seq Acc: 0.0000\n",
      " No improvement. Patience: 1/3\n",
      "\n",
      "Epoch Time: 1.0m 53.70568776130676s\n",
      "\n",
      "Epoch 3/5                                        Teacher Forcing Ratio: 0.4524\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2509/2955 [01:18<00:13, 32.14it/s, loss=2.97]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[130], line 18\u001b[0m\n\u001b[1;32m      6\u001b[0m model \u001b[38;5;241m=\u001b[39m Build_Model(\n\u001b[1;32m      7\u001b[0m     sequence_data_preprocessor\u001b[38;5;241m=\u001b[39mSequenceDataPreprocessor,\n\u001b[1;32m      8\u001b[0m     encoder\u001b[38;5;241m=\u001b[39mEncoder,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m     device\u001b[38;5;241m=\u001b[39mdevice\n\u001b[1;32m     15\u001b[0m )\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Build and train the model\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m seq2seq, train_model, evaluate_model, loss_acc_logs, _ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43memb_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrnn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_layers_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_encod_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_decod_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropout_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0001\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mteacher_force_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbidirectional\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_beam_search\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Use beam search for validation\u001b[39;49;00m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeam_width\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtesting_phase\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Set to True for testing\u001b[39;49;00m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_beam_search\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Use beam search for testing      \u001b[39;49;00m\n\u001b[1;32m     35\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[129], line 108\u001b[0m, in \u001b[0;36mBuild_Model.build\u001b[0;34m(self, emb_size, layer_type, hidden_layers_size, num_encod_layers, num_decod_layers, dropout_rate, epochs, learning_rate, teacher_force_ratio, bidirectional, save_path, patience, val_beam_search, beam_width, testing_phase, test_path, test_beam_search)\u001b[0m\n\u001b[1;32m    105\u001b[0m current_tfr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0.0\u001b[39m, current_tfr)  \u001b[38;5;66;03m# Prevent going below 0\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m40\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mTeacher Forcing Ratio: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_tfr\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m80\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 108\u001b[0m train_loss, train_char_acc, train_seq_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mteacher_force_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurrent_tfr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrain Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Train Char Acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_char_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Train Seq Acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_seq_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    113\u001b[0m val_loss, val_char_acc, val_seq_acc \u001b[38;5;241m=\u001b[39m evaluate_model\u001b[38;5;241m.\u001b[39mevaluate(beam_search\u001b[38;5;241m=\u001b[39mval_beam_search, beam_width\u001b[38;5;241m=\u001b[39mbeam_width)\n",
      "Cell \u001b[0;32mIn[116], line 58\u001b[0m, in \u001b[0;36mTrain_Model.train\u001b[0;34m(self, teacher_force_ratio)\u001b[0m\n\u001b[1;32m     54\u001b[0m acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39macc_calculator\u001b[38;5;241m.\u001b[39mcompute_accuracy(predicted_trimmed, target_trimmed)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# print(\"Sequence Accuracy:\", acc['sequence_accuracy'])\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# print(\"Character Accuracy:\", acc['character_accuracy'])\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# Add gradient clipping here\u001b[39;00m\n\u001b[1;32m     60\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseq2seq\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/VS_CODE/DA6401/DA6401_Assignment_3/predictions_vanilla/env3/lib/python3.9/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/VS_CODE/DA6401/DA6401_Assignment_3/predictions_vanilla/env3/lib/python3.9/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Set beam width (1 = greedy decoding, >1 = beam search)\n",
    "beam_width = 3\n",
    "beam_search = beam_width > 1\n",
    "\n",
    "# Initialize the model\n",
    "model = Build_Model(\n",
    "    sequence_data_preprocessor=SequenceDataPreprocessor,\n",
    "    encoder=Encoder,\n",
    "    decoder=Decoder,\n",
    "    seq2seq=Sequence2Sequence,\n",
    "    batch_size=32,\n",
    "    train_path=train_df,\n",
    "    val_path=dev_df,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Build and train the model\n",
    "seq2seq, train_model, evaluate_model, loss_acc_logs, _ = model.build(\n",
    "    emb_size=256,\n",
    "    layer_type=\"rnn\",\n",
    "    hidden_layers_size=512,\n",
    "    num_encod_layers=1,\n",
    "    num_decod_layers=2,\n",
    "    dropout_rate=0.2,\n",
    "    epochs=5,\n",
    "    learning_rate=0.0001,\n",
    "    teacher_force_ratio=0.5,\n",
    "    bidirectional=False,\n",
    "    patience=3,\n",
    "    val_beam_search=True,  # Use beam search for validation\n",
    "    beam_width=2,\n",
    "    testing_phase=True,  # Set to True for testing\n",
    "    test_path=test_df,\n",
    "    test_beam_search=True  # Use beam search for testing      \n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d260ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    'method': 'bayes',\n",
    "    'metric': {\n",
    "        'name': 'val_loss',\n",
    "        'goal': 'minimize'\n",
    "    },\n",
    "    'parameters': {\n",
    "        'emb_size': {'values': [16, 32, 64, 256]},\n",
    "        'hidden_size': {'values': [32, 64, 256]},\n",
    "        'layer_type': {'values': ['rnn', 'gru', 'lstm']},\n",
    "        'enc_layers': {'values': [1,2,3]},\n",
    "        'dec_layers': {'values': [1,2,3]},\n",
    "        'dropout': {'values': [0.2, 0.3]},\n",
    "        'learning_rate': {'values': [1e-4, 5e-4, 1e-3]},\n",
    "        'teacher_force_ratio': {'values': [0.3, 0.5, 0.7, 1.0]},\n",
    "        'epochs': {'value': 3},\n",
    "        'bidirectional': {'values': [False, True]},\n",
    "        'beam_width': {'values': [1, 3, 5]},\n",
    "        'val_beam_search': {'values': [False, True]}  \n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b73517",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_wandb_model():\n",
    "    wandb.init()\n",
    "    config = wandb.config\n",
    "\n",
    "    # Add datetime string\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "    beam_search = config.val_beam_search\n",
    "    run_name = (\n",
    "        f\"{config.layer_type}/emb{config.emb_size}-hid{config.hidden_size}-\"\n",
    "        f\"enc{config.enc_layers}-dec{config.dec_layers}-\"\n",
    "        f\"{'bi' if config.bidirectional else 'uni'}-drop{int(config.dropout * 100)}-\"\n",
    "        f\"{timestamp}\"\n",
    "    )\n",
    "    wandb.run.name = run_name\n",
    "\n",
    "    model = Build_Model(\n",
    "        sequence_data_preprocessor=SequenceDataPreprocessor,\n",
    "        encoder=Encoder,\n",
    "        decoder=Decoder,\n",
    "        seq2seq=Sequence2Sequence,\n",
    "        batch_size=16,\n",
    "        train_path=train_df,\n",
    "        val_path=dev_df,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    # Train the model and get results\n",
    "    seq2seq, train_model, evaluate_model, loss_acc_logs, _ = model.build(\n",
    "        emb_size=config.emb_size,\n",
    "        layer_type=config.layer_type,\n",
    "        hidden_layers_size=config.hidden_size,\n",
    "        num_encod_layers=config.enc_layers,\n",
    "        num_decod_layers=config.dec_layers,\n",
    "        dropout_rate=config.dropout,\n",
    "        epochs=config.epochs,\n",
    "        learning_rate=config.learning_rate,\n",
    "        teacher_force_ratio=config.teacher_force_ratio,\n",
    "        bidirectional=config.bidirectional,\n",
    "        save_path='best_model.pt',\n",
    "        patience=3,\n",
    "        val_beam_search=beam_search,\n",
    "        beam_width=config.beam_width,\n",
    "        testing_phase=False\n",
    "    )\n",
    "\n",
    "    # Log metrics to W&B\n",
    "    wandb.log({\n",
    "        \"train_loss\": loss_acc_logs[\"train_loss\"],\n",
    "        \"train_char_acc\": loss_acc_logs[\"train_char_acc\"],\n",
    "        \"train_seq_acc\": loss_acc_logs[\"train_seq_acc\"],\n",
    "        \"val_loss\": loss_acc_logs[\"val_loss\"],\n",
    "        \"val_char_acc\": loss_acc_logs[\"val_char_acc\"],\n",
    "        \"val_seq_acc\": loss_acc_logs[\"val_seq_acc\"],\n",
    "        \"beam_search\": beam_search,\n",
    "        \"beam_width\": config.beam_width\n",
    "    })\n",
    "\n",
    "    wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "2d955ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: qdxv00lp\n",
      "Sweep URL: https://wandb.ai/ed24s014-indian-institute-of-technology-madras/test_seq2seq_without_Attention/sweeps/qdxv00lp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: nwbwi8v2 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_width: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbidirectional: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_layers: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \temb_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_layers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlayer_type: gru\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tteacher_force_ratio: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tval_beam_search: True\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "creating run (0.0s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/indramandal/Documents/VS_CODE/DA6401/DA6401_Assignment_3/predictions_vanilla/wandb/run-20250504_175642-nwbwi8v2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ed24s014-indian-institute-of-technology-madras/test_seq2seq_without_Attention/runs/nwbwi8v2' target=\"_blank\">morning-sweep-1</a></strong> to <a href='https://wandb.ai/ed24s014-indian-institute-of-technology-madras/test_seq2seq_without_Attention' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/ed24s014-indian-institute-of-technology-madras/test_seq2seq_without_Attention/sweeps/qdxv00lp' target=\"_blank\">https://wandb.ai/ed24s014-indian-institute-of-technology-madras/test_seq2seq_without_Attention/sweeps/qdxv00lp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ed24s014-indian-institute-of-technology-madras/test_seq2seq_without_Attention' target=\"_blank\">https://wandb.ai/ed24s014-indian-institute-of-technology-madras/test_seq2seq_without_Attention</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/ed24s014-indian-institute-of-technology-madras/test_seq2seq_without_Attention/sweeps/qdxv00lp' target=\"_blank\">https://wandb.ai/ed24s014-indian-institute-of-technology-madras/test_seq2seq_without_Attention/sweeps/qdxv00lp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ed24s014-indian-institute-of-technology-madras/test_seq2seq_without_Attention/runs/nwbwi8v2' target=\"_blank\">https://wandb.ai/ed24s014-indian-institute-of-technology-madras/test_seq2seq_without_Attention/runs/nwbwi8v2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 188,608 trainable parameters\n",
      "\n",
      "Epoch 1/3                                        Teacher Forcing Ratio: 0.3000\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches:   4%|â–         | 224/5910 [00:03<01:29, 63.71it/s, loss=2.78]\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n",
      "Training Batches:   4%|â–         | 227/5910 [00:03<01:30, 62.75it/s, loss=2.78]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Problem finishing run\n",
      "Exception in thread Thread-23:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/indramandal/Documents/VS_CODE/DA6401/DA6401_Assignment_3/predictions_vanilla/env3/lib/python3.9/site-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "    self._function()\n",
      "  File \"/var/folders/16/2v843_gd4x1cty4mwj7r2t780000gn/T/ipykernel_48644/3870340499.py\", line 25, in train_wandb_model\n",
      "  File \"/var/folders/16/2v843_gd4x1cty4mwj7r2t780000gn/T/ipykernel_48644/3479093571.py\", line 108, in build\n",
      "  File \"/var/folders/16/2v843_gd4x1cty4mwj7r2t780000gn/T/ipykernel_48644/544483360.py\", line 37, in train\n",
      "  File \"/Users/indramandal/Documents/VS_CODE/DA6401/DA6401_Assignment_3/predictions_vanilla/env3/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/Users/indramandal/Documents/VS_CODE/DA6401/DA6401_Assignment_3/predictions_vanilla/env3/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/var/folders/16/2v843_gd4x1cty4mwj7r2t780000gn/T/ipykernel_48644/329109780.py\", line 124, in forward\n",
      "  File \"/Users/indramandal/Documents/VS_CODE/DA6401/DA6401_Assignment_3/predictions_vanilla/env3/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/Users/indramandal/Documents/VS_CODE/DA6401/DA6401_Assignment_3/predictions_vanilla/env3/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/var/folders/16/2v843_gd4x1cty4mwj7r2t780000gn/T/ipykernel_48644/329109780.py\", line 72, in forward\n",
      "  File \"/Users/indramandal/Documents/VS_CODE/DA6401/DA6401_Assignment_3/predictions_vanilla/env3/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/Users/indramandal/Documents/VS_CODE/DA6401/DA6401_Assignment_3/predictions_vanilla/env3/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/Users/indramandal/Documents/VS_CODE/DA6401/DA6401_Assignment_3/predictions_vanilla/env3/lib/python3.9/site-packages/torch/nn/modules/sparse.py\", line 162, in forward\n",
      "    return F.embedding(\n",
      "  File \"/Users/indramandal/Documents/VS_CODE/DA6401/DA6401_Assignment_3/predictions_vanilla/env3/lib/python3.9/site-packages/torch/nn/functional.py\", line 2233, in embedding\n",
      "    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)\n",
      "Exception\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/threading.py\", line 973, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/Users/indramandal/Documents/VS_CODE/DA6401/DA6401_Assignment_3/predictions_vanilla/env3/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/threading.py\", line 910, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/indramandal/Documents/VS_CODE/DA6401/DA6401_Assignment_3/predictions_vanilla/env3/lib/python3.9/site-packages/wandb/agents/pyagent.py\", line 311, in _run_job\n",
      "    wandb.finish(exit_code=1)\n",
      "  File \"/Users/indramandal/Documents/VS_CODE/DA6401/DA6401_Assignment_3/predictions_vanilla/env3/lib/python3.9/site-packages/wandb/sdk/wandb_run.py\", line 4334, in finish\n",
      "    wandb.run.finish(exit_code=exit_code, quiet=quiet)\n",
      "  File \"/Users/indramandal/Documents/VS_CODE/DA6401/DA6401_Assignment_3/predictions_vanilla/env3/lib/python3.9/site-packages/wandb/sdk/wandb_run.py\", line 406, in wrapper\n",
      "    return func(self, *args, **kwargs)\n",
      "  File \"/Users/indramandal/Documents/VS_CODE/DA6401/DA6401_Assignment_3/predictions_vanilla/env3/lib/python3.9/site-packages/wandb/sdk/wandb_run.py\", line 503, in wrapper\n",
      "    return func(self, *args, **kwargs)\n",
      "  File \"/Users/indramandal/Documents/VS_CODE/DA6401/DA6401_Assignment_3/predictions_vanilla/env3/lib/python3.9/site-packages/wandb/sdk/wandb_run.py\", line 451, in wrapper\n",
      "    return func(self, *args, **kwargs)\n",
      "  File \"/Users/indramandal/Documents/VS_CODE/DA6401/DA6401_Assignment_3/predictions_vanilla/env3/lib/python3.9/site-packages/wandb/sdk/wandb_run.py\", line 2302, in finish\n",
      "    return self._finish(exit_code)\n",
      "  File \"/Users/indramandal/Documents/VS_CODE/DA6401/DA6401_Assignment_3/predictions_vanilla/env3/lib/python3.9/site-packages/wandb/sdk/wandb_run.py\", line 406, in wrapper\n",
      "    return func(self, *args, **kwargs)\n",
      "  File \"/Users/indramandal/Documents/VS_CODE/DA6401/DA6401_Assignment_3/predictions_vanilla/env3/lib/python3.9/site-packages/wandb/sdk/wandb_run.py\", line 2330, in _finish\n",
      "    self._atexit_cleanup(exit_code=exit_code)\n",
      "  File \"/Users/indramandal/Documents/VS_CODE/DA6401/DA6401_Assignment_3/predictions_vanilla/env3/lib/python3.9/site-packages/wandb/sdk/wandb_run.py\", line 2543, in _atexit_cleanup\n",
      "    self._on_finish()\n",
      "  File \"/Users/indramandal/Documents/VS_CODE/DA6401/DA6401_Assignment_3/predictions_vanilla/env3/lib/python3.9/site-packages/wandb/sdk/wandb_run.py\", line 2790, in _on_finish\n",
      "    exit_handle = self._backend.interface.deliver_exit(self._exit_code)\n",
      "  File \"/Users/indramandal/Documents/VS_CODE/DA6401/DA6401_Assignment_3/predictions_vanilla/env3/lib/python3.9/site-packages/wandb/sdk/interface/interface.py\", line 1006, in deliver_exit\n",
      "    return self._deliver_exit(exit_data)\n",
      "  File \"/Users/indramandal/Documents/VS_CODE/DA6401/DA6401_Assignment_3/predictions_vanilla/env3/lib/python3.9/site-packages/wandb/sdk/interface/interface_shared.py\", line 436, in _deliver_exit\n",
      "    return self._deliver_record(record)\n",
      "  File \"/Users/indramandal/Documents/VS_CODE/DA6401/DA6401_Assignment_3/predictions_vanilla/env3/lib/python3.9/site-packages/wandb/sdk/interface/interface_shared.py\", line 389, in _deliver_record\n",
      "    handle = mailbox.require_response(record)\n",
      "  File \"/Users/indramandal/Documents/VS_CODE/DA6401/DA6401_Assignment_3/predictions_vanilla/env3/lib/python3.9/site-packages/wandb/sdk/mailbox/mailbox.py\", line 68, in require_response\n",
      "    raise MailboxClosedError()\n",
      "wandb.sdk.mailbox.mailbox.MailboxClosedError\n"
     ]
    }
   ],
   "source": [
    "sweep_id = wandb.sweep(sweep_config, project=\"test_seq2seq_without_Attention\")\n",
    "wandb.agent(sweep_id, function=train_wandb_model, count=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "13402f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc84dc7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
